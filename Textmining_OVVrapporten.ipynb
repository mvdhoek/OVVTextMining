{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining met Python - OVV Rapporten\n",
    "\n",
    "|   |   |\n",
    "|------|------|\n",
    "|__Auteur__ | Marlon van den Hoek  |\n",
    "| __E-mail__  | <m.vandenhoek@onderzoeksraad.nl>  |\n",
    "| __Onderwerpen__ | text mining, cleansing, tokenizing, stemming, stopwords, TF-IDF, Jaccard similarity, KMeans, hierarchical document clustering, LDA, sentiment analysis |\n",
    "| __Afhankelijkheden__ | `nltk`, `textmining`, `pdfminer`, `re`, `multiprocessing`, `sklearn`, `pandas`, `gensim` |\n",
    "| __Versie__ | `0.6` |\n",
    "\n",
    "_Opmerking 1_: Om dit Jupyter Notebook correct te kunnen runnen moeten eerst de juiste pakketten (zie afhankelijkheden) geinstalleerd worden. Dit kan bijvoorbeeld met `pip`. Zie hiervoor de eerste cell hier onder.\n",
    "\n",
    "_Opmerking 2_: Dit notebook is standaard beperkt tot 30 documenten. Om alle documenten te analyseren, verwijder de limiet van 30 bij het laden: verander `arange` bij de for loop, `job` bij de multicore loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the dependencies, run the following code -- run only once!!\n",
    "! pip install nltk\n",
    "! pip install textmining\n",
    "! pip install pdfminer\n",
    "! pip install multiprocessing\n",
    "! pip install sklearn\n",
    "! pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie\n",
    "\n",
    "In dit Jupyter notebook gaan we aan de slag met text mining op de (openbare) rapporten van de Onderzoeksraad. Met een ander script (scraper) zijn meer dan 600 rapporten van de OVV website gedownload. Daarin is rekening gehouden met andere documenten zoals errata, bijlagen, aanbevelingen; deze documenten zijn voor het grootste gedeelte uit de collectie gefilterd. Deze documenten zijn opgeslagen in `./Rapporten/`.\n",
    "\n",
    "Omdat de meeste rapporten van de OVV zijn voorzien van een beveiliging tegen het kopieren van tekst, zijn alle rapporten eerst gedecrypt met een extern programma. De gedecrypte bestanden zijn te vinden in de map `./Rapporten decrypted/`. De onderliggende submappen geven de taal van het document aan: Nederlands `nl` of Engels `en`.\n",
    "\n",
    "Voor het minen maken we gebruik van de natural language toolkit ([NLTK](http://www.nltk.org/)), het textmining pakket ([textmining1.0](https://pypi.python.org/pypi/textmining/1.0)) en het boek _Natural Language Processing with Python_<sup id=\"a1\">[1](#f1)</sup>. Daarnaast maken we ook gebruik van pakketten voor lineaire algebra en matrix operaties, e.a. Deze pakketten importeren we eerst.\n",
    "\n",
    "<b id=\"f1\">1</b> Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \"O'Reilly Media, Inc.\". [â†©](#a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import nltk, textmining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text laden\n",
    "\n",
    "Als we gebruik willen maken van onze eigen corpus, database met text, dan moeten we de rapporten die gedownload zijn in pdf eerst converteren naar plain text (txt). Hiervoor gebruiken we `pdfminer`. Metadata halen we uit de pdf bestanden met het pakket `pyPdf` omdat deze sneller is.\n",
    "\n",
    "In de volgende code importeren we eerst alle pakketten die nodig zijn, daarna definieren we de bestandlocatie en creeeren we een functie om een bestand te kunnen lezen. In deze functie wordt elke pagina van het desbetreffende bestand geparsed en bij elkaar gevoegd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys\n",
    "import os\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from cStringIO import StringIO\n",
    "from pyPdf import PdfFileReader # extract pdf metadata\n",
    "\n",
    "# Define language\n",
    "lang = 'en'\n",
    "\n",
    "# Define location of reports\n",
    "FilePath = './Rapporten decrypted/' + lang + '/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames = os.listdir(FilePath)\n",
    "\n",
    "# Make function to convert pdf to text\n",
    "def pdfparser(FilePath,FileName): \n",
    "    fp = file(FilePath+FileName, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    # Process each page contained in the document.\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    \n",
    "    # return text\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de functie om een bestand te lezen gedefineerd is kunnen we deze functie toe gaan passen op alle bestanden. We doen dit door over de bestanden heen te loopen.\n",
    "\n",
    "In de loop gebeurt het volgende: \n",
    "1. het bestand wordt gelezen\n",
    "2. de volledige, onveranderde tekst wordt toegevoegd aan matrix `fulltextmat` om later te gebruiken. Sommige functies vragen een ander tekstformaat, daarom slaan we de tekst op verschillende manieren op.\n",
    "3. met het Regexp `re` pakket splitten we de tekst\n",
    "4. converteren de tekst naar lower case\n",
    "5. uitfilteren van lege indices\n",
    "6. uitfilteren van numerieke waarden\n",
    "7. als de overgebleven body niet `None` is dan voegen we de tokens toe aan de `textbody` array (alle woorden achter elkaar), slaan de tokens op in een dict `textdict` en als laatste voegen we de tokens van een tekst ook nog toe aan de matrix `textmat`. \n",
    "\n",
    "_**Let op**_ om geparallelliseerd bestanden te lezen en converteren, sla de volgende stappen over en ga naar [hier](#parallel). Als je systeem voorzien is van meerdere cores kan dit significant sneller zijn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regexp package to split\n",
    "import re\n",
    "\n",
    "#textbody = np.array([])\n",
    "#fulltextmat = []   # update: niet meer nodig! laten staan ivm fallback\n",
    "textdict = {}\n",
    "metadict = {}\n",
    "#textmat = []\n",
    "#for i in np.arange(np.shape(FileName)[0]): # use this line for all files\n",
    "for i in np.arange(0,30): # temp only take first 30 files\n",
    "    print \"Processing \",i+1,'/',np.shape(FileNames)[0],': ', FileNames[i],'\\n'\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath,FileNames[i])\n",
    "        \n",
    "        # add to fulltextmat\n",
    "        #fulltextmat.append(data)\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath,FileNames[i]]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict[FileNames[i]] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        # filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        # filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "\n",
    "            # add to textbody array\n",
    "            #textbody = np.append(textbody,data)\n",
    "        \n",
    "            # also add data to dict and matrix for later use\n",
    "            textdict[FileNames[i]] = list(data)\n",
    "            #textmat.append(list(data))\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileNames[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geparallelliseerd bestanden lezen en converteren\n",
    " <a name=\"parallel\"></a>\n",
    " \n",
    "Om een groot gedeelte van de `for` loops sneller te maken, kunnen we de processen parallelliseren met `multiprocessing`. Dit pakket moeten we eerst importeren en dan definieren wat we ermee willen doen.\n",
    "\n",
    "Om het uitlezen van bestanden te kunnen faciliteren moeten we ook eerst een functie definieren om een bestand te lezen en cleanen, deze functie noemen we _processandtokenize_. Deze functie wordt parallel uitgevoerd en de output is een dict met daarin al onze bestandnamen en bijbehorende tokens.\n",
    "\n",
    "_Let op_: het runnen van onderstaande code is erg CPU intensief en kan ervoor zorgen dat je systeem langzaam wordt gedurende het processen van de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "# set number of processing cores to <max_cores> - 1\n",
    "num_cores = multiprocessing.cpu_count() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing:  1 / 221 1f850875f33erapport-treinontsporing-hilversum-en-interactief.pdf\n",
      "Processing:  2 / 221 2bf2ed16223ab-rapport-annelies-ilena-en.pdf\n",
      "Processing:  3 / 221 2e46783a2333rapport-direct-teruggekeerd-na-rook-in-cockpit-bw-en-web.pdf\n",
      "Processing:  4 / 221 4a5183767e76b-rapport-runway-incursion-en.pdf\n",
      "Processing:  5 / 221 4d4b4bc7f9ddb-rapport-verlies-van-controle-tijdens-doorstart-en.pdf\n",
      "Processing:  6 / 221 4d4057d68fe4ovv-semester-scheepvaartongevallen-5-en.pdf\n",
      "Processing:  7 / 221 5c4ffd1921dbb-rapport-diamond-da-40d-en-interactief.pdf\n",
      "Processing:  8 / 221 6a2c806849e0report-mh17-brochure-crash.pdf\n",
      "Processing:  9 / 221 7df8df730b98publieksversie-azoresborg-web.pdf\n",
      "Processing:  10 / 221 7f249d3143fepublieksuitgave-flinter-aland-en-web.pdf\n",
      "Processing:  11 / 221 8a23d852303d20172585-b-rapport-atlantic-dawn-en-170823.pdf\n",
      "Processing:  12 / 221 8ac71529b0892012037-rapport-overig-verkeer-in-nabijheid-en-def.pdf\n",
      "Processing:  14 / 221 9fca1f1c7f3fb-rapport-aanvaring-in-ankergebied-en.pdf\n",
      "Processing:  13 / 221 09ab8c4c51f0b-rapport-beknelling-luikenwagen-en.pdf\n",
      "Processing:  16 / 221 10f63dcb84c9rapport-cessna-maasvlakte-en.pdf\n",
      "Processing:  17 / 221 011e_LV_1996-12-A-5_G-JTCA_Piper_PA23_De_Kooij.pdf\n",
      "Processing:  15 / 221 010e_LV_1994-02_G-ZIPP_Cessna_310Q_Rotterdam.pdf\n",
      "Processing:  18 / 221 017e_LV_1997-74-A-25_PH-KHB_Sikorsky_S-76B_bij_Den_Helder.pdf\n",
      "Processing:  20 / 221 018e_LV_1997-75-A-26_PH-TKC_Boeing_757_Transavia_Schiphol.pdf\n",
      "Processing:  22 / 221 29b38bbda9afb-rapport-koolmonoxide-in-boegschroefruimte-en.pdf\n",
      "Processing:  19 / 221 18c5e705b0c42012041rapport-bijna-botsing-en-def.pdf\n",
      "Processing:  21 / 221 21ed847e31beeam017776-1-rev-2.pdf\n",
      "Processing:  23 / 221 029e_LV_1998-85-S-14_N-193DN_Boeing_767_Delta_Schiphol.pdf\n",
      "Processing:  25 / 221 046e_LV_2000141_G-BWZE_P84_Jet_Provost_Lelystad.pdf\n",
      "Processing:  24 / 221 42a34fdbd5b2report-mh17-abouttheinvestigation.pdf\n",
      "Processing:  26 / 221 056b43453198rapport-flinter-aland-en-web-def.pdf\n",
      "Processing:  27 / 221 059e_LV_2002083_D-EHAE_Short_report.pdf\n",
      "Processing:  29 / 221 074e_RV_Fire_in_high-speed_tram.pdf\n",
      "Processing:  30 / 221 075e_RV_Passenger_train_derailment.pdf\n",
      "Processing:  28 / 221 064e_LV_2002124_PH-MLH_New_Piper_PA-44-180_Kampen.pdf\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def processandtokenize(FileName, dictname, metadict): \n",
    "    \n",
    "    # print progress, var FileNames is taken from shared memory, not required as function input\n",
    "    print \"Processing: \",int(np.arange(0,len(FileNames))[np.array(FileNames) == FileName])+1,'/',len(FileNames), FileName\n",
    "    #print \"Processing \", FileName\n",
    "    \n",
    "    try:\n",
    "        FilePath = './Rapporten decrypted/'+lang+'/'\n",
    "        data = pdfparser(FilePath,FileName)\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath,FileName]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict[FileName] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        ## filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        ## filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "        # add data to dict\n",
    "        dictname[FileName] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileName\n",
    "        \n",
    "\n",
    "# initialize multi-core process\n",
    "manager = multiprocessing.Manager()\n",
    "textdict = manager.dict()\n",
    "metadict = manager.dict()\n",
    "\n",
    "# define job\n",
    "job = [multiprocessing.Process(target=processandtokenize, args=(i, textdict, metadict)) for i in FileNames[0:30]] # first 30 files\n",
    "#job = [multiprocessing.Process(target=processandtokenize, args=(i, textdict, metadict)) for i in FileNames] # for all files\n",
    "\n",
    "\n",
    "# start all jobs in the job list\n",
    "_ = [p.start() for p in job]\n",
    "_ = [p.join() for p in job]\n",
    "\n",
    "# convert the multiprocess textdict opject to normal dict\n",
    "textdict = dict(textdict)\n",
    "metadict = dict(metadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  195856\n"
     ]
    }
   ],
   "source": [
    "print 'Total number of tokens: ',np.shape(sum(list(textdict.values()),[]))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Text stemming\n",
    "\n",
    "Nu alle woorden uit de text gehaald zijn en in een `numpy` array staan kunnen we de woorden gaan _stemmen_. We gebruiken hiervoor de Porter stemmer voor het Engels en de Snowball stemmer in de NLTK toolkit voor het Nederlands.\n",
    "\n",
    "Let op: de huidige implementatie van de Snowball stemmer is niet erg doeltreffend voor de Nederlandse taal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "print \"Supported Languages Snowball: \\n\"\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Supported Languages Porter: \\n\"\n",
    "print \"english\\n\"\n",
    "\n",
    "print \"Processing \", lang, \"stopwords\\n\"\n",
    "\n",
    "# create a new instance\n",
    "if lang == 'nl':\n",
    "    stemmer = SnowballStemmer(\"dutch\")\n",
    "elif lang == 'en':\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "    text_stemmed = []\n",
    "    print i+1,'/',np.shape(textdict.keys())[0]\n",
    "    for j in np.arange(0,np.shape(textdict.values()[i])[0]):\n",
    "        text_stemmed.append(stemmer.stem(textdict.values()[i][j]))\n",
    "    textdict[textdict.keys()[i]] = text_stemmed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords verwijderen\n",
    "\n",
    "Om onze teksten te processen moeten we nog een cleansing stap uitvoeren: het verwijderen van de stopwoorden. Hiervoor gebruiken we de standaard NL/EN library in de NLTK toolkit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "# filter NL en EN stopwoorden uit de tekst\n",
    "#if lang == 'en':\n",
    "#    data_filtered = np.array([w for w in data_filtered if not w in stopwords.words('english')])\n",
    "#elif lang == 'nl':\n",
    "#    data_filtered = np.array([w for w in textbody if not w in stopwords.words('dutch')])\n",
    "\n",
    "for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "    print i+1,'/',np.shape(textdict.keys())[0]\n",
    "    text_nostopw = [w for w in textdict.values()[i] if not w in stopwords.words('english') and not w in stopwords.words('dutch')]\n",
    "      #  text_nostopw = [w for w in textdict.values()[i] if not w in stopwords.words('dutch') ]\n",
    "    textdict[textdict.keys()[i]] = text_nostopw\n",
    "\n",
    "# trick to flatten matrix: sum(list(textdict.values()),[])\n",
    "count = Counter(sum(list(textdict.values()),[]))\n",
    "\n",
    "print count.most_common(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Jaccard Similarity\n",
    "\n",
    "Om verschillende documenten met elkaar te vergelijken maken we gebruik van de _Jaccard Similarity_: $J\\left(A,B\\right) = \\frac{|A\\bigcap B|}{|A\\bigcup B|}$. Dit is de lengte van de intersectie van de set tokens van tekst A en tekst B gedeeld door de union van beide sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Jaccard similarity\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return float(len(intersection))/len(union)\n",
    "\n",
    "# calculate the Jaccard similarity of text 1 and text 2\n",
    "print jaccard_similarity(textdict.values()[0],textdict.values()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De uitkomst van de Jaccard Similarity geeft de numerieke overeenkomst van beide teksten. In het geval veel stopwoorden in de tekst te vinden zijn is de kans groot dat hierdoor de relevantie omhoog geholpen wordt. Het is daarom belangrijk, alvorens de Jaccard Similarity uit te rekenen, om de stopwords te verwijderen. \n",
    "\n",
    "Om de intersectie van beide teksten te bekijken, gebruik het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the intersection of both texts:\n",
    "set(textdict.values()[0]).intersection(set(textdict.values()[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - _term frequency-inverse document frequency_\n",
    "\n",
    "Om het belang van verschillende woorden in een corpus numeriek uit te drukken berekenen we de TF-IDF. Dit doen we door gebruik te maken van de functie `feature_extraction` in de `sklearn` toolbox (onderdeel van `scikit`). \n",
    "\n",
    "De TF-IDF measure reflecteert hoe belangrijk een woord voor een document is door in eerste instantie te kijken hoe vaak een woord voor komt in een tekst. In het meest simpele geval gebruiken we hiervoor de woordfrequentie $f_{t,d}$. Ons algorimte maakt gebruik van de geaugmenteerde woordfrequentie $tf$:\n",
    "\n",
    "$\\text{tf}(t,d) = 0.5 + \\frac{0.5 ~\\times ~f(t,d)}{\\text{max}\\left\\{ f(w,d):~w~\\in ~d\\right\\}}$\n",
    "\n",
    "waarin de noemer de maximale woordfrequentie aangeeft.\n",
    "\n",
    "Omdat sommige woorden vaak in tekst aanwezig zijn maar eigenlijk weinig betekenis hebben maken we gebruik van de inverse document frequentie om de uitkomst van de woord frequentie te reguleren. De IDF is gedefinieerd als:\n",
    "\n",
    "$ \\text{idf}(t) = \\log \\frac{n_d}{df\\left( d,t \\right)} + 1$\n",
    "\n",
    "hierin is $n_d$ het totaal aantal documenten en $df\\left(t,d\\right)$ het aantal documenten dat term $t$ bevat.\n",
    "\n",
    "De TF-IDF krijgen we dan door:\n",
    "\n",
    "$ \\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times \\text{idf}(t)$\n",
    "\n",
    "De TF-IDF wordt berekend voor alle unieke termen $t$ in alle unieke documenten $d$. De resulterende matrix heeft dus de dimensie $\\left( t,d \\right)^{\\intercal}$ waarin element ($t_i,d_j)^{\\intercal}$ de numerieke relevantie geeft van term $i$ in document $j$.\n",
    "\n",
    "__**Limitatie bag of words**__\n",
    "Omdat een simpel bag of words model geen rekening houdt met verkeerd gespelde woorden/typo's kunnen deze woorden als verschillend worden gezien door het algoritme. Om dit te voorkomen kunnen we gebruik maken van een collectie bigrams (n=2) of ngrams (n>2). Met een analyzer (in dit geval `char_wb`) en een gedefinieerde `ngram_range` kunnen we woorden dan herkennen aan $n$ features. Zo kunnen we voorkomen dat simpele fouten invloed hebben op het model.\n",
    "\n",
    "**Let op**: hoe meer ngrams features, hoe meer geheugen nodig is!\n",
    "\n",
    "Omdat `Tfidfvectorizer` alleen de volledige tekst als string accepteert moeten we eerst de tokens in `textdict` manipuleren -> we voegen de tokens samen tot een string en splitten die dan weer op de spatie tussen woorden. Dit is een workaround en genereert een kleine overhead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# manipulate variable textdict to allow feeding into TfidfVectorizer function\n",
    "alltext = [' '.join(text) for text in textdict.values()]\n",
    "\n",
    "# define tokenizer\n",
    "# dummy function: we split concatenated tokens at space\n",
    "tfidf_tokenizer = lambda doc: [word for word in doc.split(' ')]\n",
    "\n",
    "# define numer of ngrams\n",
    "ngrams = (2,2)\n",
    "\n",
    "# define vectorizer -- using ngrams\n",
    "#sklearn_tfidf = TfidfVectorizer(analyzer='char_wb',ngram_range=ngrams,norm='l2',min_df=0,use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "# define vectorizer -- unigram\n",
    "sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0,use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "tfidf_matrix = sklearn_tfidf.fit_transform(alltext)\n",
    "\n",
    "print tfidf_matrix.toarray()[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(zie ook http://billchambers.me/tutorials/2014/12/21/tf-idf-explained-in-python.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to count occurrence of every token\n",
    "#def term_frequency(term, tokenized_document):\n",
    "#    return tokenized_document.count(term)\n",
    "#\n",
    "#def sublinear_term_frequency(term, tokenized_document):\n",
    "#    return 1 + math.log(tokenized_document.count(term))\n",
    "#\n",
    "#def augmented_term_frequency(term, tokenized_document):\n",
    "#    max_count = max([term_frequency(t, tokenized_document) for t in tokenized_document])\n",
    "#    return (0.5 + ((0.5 * term_frequency(term, tokenized_document))/max_count))\n",
    "#\n",
    "#def inverse_document_frequencies(tokenized_documents):\n",
    "#    idf_values = {}\n",
    "#    all_tokens_set = set([item for sublist in tokenized_documents for item in sublist])\n",
    "#    for tkn in all_tokens_set:\n",
    "#        contains_token = map(lambda doc: tkn in doc, tokenized_documents)\n",
    "#        idf_values[tkn] = 1 + math.log(len(tokenized_documents)/(sum(contains_token)))\n",
    "#    return idf_values\n",
    "#\n",
    "#def tfidf(tokenized_documents):\n",
    "#    idf = inverse_document_frequencies(tokenized_documents)\n",
    "#    tfidf_documents = []\n",
    "#    for document in tokenized_documents:\n",
    "#        doc_tfidf = []\n",
    "#        for term in idf.keys():\n",
    "#            tf = sublinear_term_frequency(term, document)\n",
    "#            doc_tfidf.append(tf * idf[term])\n",
    "#        tfidf_documents.append(doc_tfidf)\n",
    "#    return tfidf_documents\n",
    "\n",
    "\n",
    "# OLD !!!!!!! DO NOT REMOVE !!!!!!!\n",
    "\n",
    "# sklearn implementation\n",
    "\n",
    "# define a tokenizer function\n",
    "#from nltk.corpus import stopwords\n",
    "#tokenize = lambda doc: [word for word in re.split('\\W+', doc.lower()) if np.char.isalpha(word) \n",
    "#                         and len(word)>1 and word not in stopwords.words('dutch') and word not in\n",
    "#                         stopwords.words('english')]\n",
    "#\n",
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0,use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tokenize)\n",
    "#\n",
    "#tfidf_matrix = sklearn_tfidf.fit_transform(fulltextmat)\n",
    "#\n",
    "#print tfidf_matrix.toarray()[0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = sklearn_tfidf.get_feature_names()\n",
    "print terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Aan de hand van de TF-IDF matrix kunnen we de cosine similarity uitrekenen en de onderlinge (numerieke) afstand tussen verschillende text bestanden. Op basis van deze measure kan er dan met het KMeans algoritme een onderscheid gemaakt worden tussen verschillende clusters.\n",
    "\n",
    "Het aantal clusters moeten we zelf definieren. Dit is onderhevig aan trial en error: het zoeken van het optimale aantal clusters is geen triviaal proces en wordt vaak aangepakt met een brute force aanpak. Dit komt dus neer op $n$ keer het algoritme zijn clusters laten bepalen voor $n$ verschillende clustergrootten en achteraf bepalen met welke clustergrootte het beste resultaat wordt bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 5\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# print clusters\n",
    "for i in np.arange(0,num_clusters):\n",
    "    print \"Cluster\", i, \": \\n\"\n",
    "    print np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "clusters = km.labels_.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "MDS()\n",
    "\n",
    "# dimension reduction -> reduce multi-dimensional field to 2 dimensions for plotting\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:,0], pos[:,1]\n",
    "\n",
    "# visualize clusters\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=textdict.keys())) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "cluster_names = ['Cluster '+str(w) for w in np.arange(np.min(clusters),np.max(clusters)+1,1)]\n",
    "cluster_colors = ['red','green','blue','black','orange']*5\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=8)  \n",
    "\n",
    "    \n",
    "    \n",
    "plt.show() #show the plot\n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "#plt.savefig('clusters_small_noaxes.png', dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical document clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15./60*len(textdict.keys())))\n",
    "\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=textdict.keys());\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "plt.savefig('Rapport-dendogram_plot.pdf', dpi=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Topic Modeling\n",
    "\n",
    "Om meer van de verborgen structuur in de tekstdocumenten te vinden kunnen we gebruik maken van _Latend Dirichlet Allocation_. LDA is een propabilistisch topic model die aanneemt dat documenten een mix zijn van verschillende onderwerpen (topics) en dat elk woord in het document bijdraagt aan dit onderwerp (en zo komt het onderwerp tot stand).\n",
    "\n",
    "Om LDA te gebruiken moeten we eerst een model trainen met de data (teksten) die we hebben.\n",
    "\n",
    "Eerst importeren we de benodigde toolkits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken nu een dictionary van alle tokens in onze `textdict` variabele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(textdict.values())\n",
    "\n",
    "# remove extremes\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "# convert dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in textdict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the LDA model - TAKES A LONG TIME! (30 docs = +-1 min)\n",
    "%time lda = models.LdaModel(corpus, num_topics=5,id2word = dictionary,update_every=5,chunksize=100000,passes=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=20)\n",
    "\n",
    "# show topics\n",
    "topics_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analyse\n",
    "\n",
    "Om het sentiment van een document in te kunnen schatten kunnen we gebruik maken van _Naive Bayes_. Voor dit model wordt tekst gerepresenteerd als _bag of words_. De nauwkeurigheid van deze methode is afhankelijk van de lexicologie we gebruiken. `NLTK` maakt gebruik van een eigen library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = np.array([])\n",
    "\n",
    "# convert metadict date field to date array\n",
    "for i in np.arange(0,len(metadict)):\n",
    "    year = metadict[metadict.keys()[i]]['/CreationDate'][2:6]\n",
    "    month = metadict[metadict.keys()[i]]['/CreationDate'][6:8]\n",
    "    day = metadict[metadict.keys()[i]]['/CreationDate'][8:10]\n",
    "\n",
    "    # convert to python datetime object\n",
    "    date = np.append(date,datetime.date(int(year), int(month), int(day)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as pltdates\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# set plot properties\n",
    "plt.title(\"Tijdlijn rapporten\", fontsize=22)\n",
    "fig.set_size_inches(20,20./60*len(textdict.keys()))\n",
    "fig.gca().spines['left'].set_visible(False)\n",
    "fig.gca().spines['right'].set_visible(False)\n",
    "fig.gca().spines['top'].set_visible(False)\n",
    "fig.gca().get_yaxis().set_ticks([]) # remove y ticks\n",
    "fig.gca().axes.yaxis.set_ticklabels([]) # remove y labels\n",
    "plt.grid(True,which='major',axis='x')\n",
    "\n",
    "# plot data\n",
    "xplot = np.arange(0,np.shape(textdict.keys())[0])\n",
    "yind = np.argsort(date) # sort date for plotting\n",
    "plt.scatter(date[yind], xplot, c=(75./256,151./256,182./256),s=100) \n",
    "for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "    plt.text(date[yind[i]]+datetime.timedelta(60), i, metadict.keys()[yind[i]], fontsize=8)\n",
    "    \n",
    "plt.xticks(np.arange(datetime.date(min(date).year,1,1),datetime.date(max(date).year+1,1,1),365),fontsize=16,rotation=70)\n",
    "    \n",
    "# make sure text fits in axis\n",
    "plt.xlim(fig.gca().get_xlim()[0],fig.gca().get_xlim()[1]*1.003)\n",
    "\n",
    "# show figure and save figure to working directory\n",
    "plt.savefig('Rapport-datum_plot.pdf', dpi=80, orientation='landscape')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEE\n",
    "\n",
    "Tekst met keywords in pdf markeren, zie voorbeelden om locatie van het woord te extracten en de tekst in het PDF bestand te highlighten:\n",
    "\n",
    "- https://stackoverflow.com/questions/7605577/read-highlight-save-pdf-programatically\n",
    "- https://stackoverflow.com/questions/22898145/how-to-extract-text-and-text-coordinates-from-a-pdf-file/\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
