{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining met Python - OVV Rapporten\n",
    "\n",
    "|   |   |\n",
    "|------|------|\n",
    "|__Auteur__ | Marlon van den Hoek  |\n",
    "| __E-mail__  | <m.vandenhoek@onderzoeksraad.nl>  |\n",
    "| __Onderwerpen__ | text mining, cleansing, tokenizing, stemming, stopwords, TF-IDF, Jaccard similarity, KMeans, hierarchical document clustering, LDA, sentiment analysis |\n",
    "| __Afhankelijkheden__ | `nltk`, `textmining`, `pdfminer`, `re`, `multiprocessing`, `sklearn`, `pandas`, `gensim`, `mpld3` (alleen voor plotting) |\n",
    "|__Datum__ | 14-02-2018 |\n",
    "| __Versie__ | `0.7` |\n",
    "\n",
    "_Opmerking 1_: Om dit Jupyter Notebook correct te kunnen runnen moeten eerst de juiste pakketten (zie afhankelijkheden) geinstalleerd worden. Dit kan bijvoorbeeld met `pip`. Zie hiervoor de eerste cell hier onder.\n",
    "\n",
    "_Opmerking 2_: Dit notebook is standaard beperkt tot 30 documenten. Om alle documenten te analyseren, verwijder de limiet van 30 bij het laden: verander `arange` bij de for loop, `job` bij de multicore loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the dependencies, run the following code -- run only once!!\n",
    "! pip install nltk\n",
    "! pip install textmining\n",
    "! pip install pdfminer\n",
    "! pip install multiprocessing\n",
    "! pip install sklearn\n",
    "! pip install gensim\n",
    "! pip install mpld3 # for d3.js plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie\n",
    "\n",
    "In dit Jupyter notebook gaan we aan de slag met text mining op de (openbare) rapporten van de Onderzoeksraad. Met een ander script (scraper) zijn meer dan 600 rapporten van de OVV website gedownload. Daarin is rekening gehouden met andere documenten zoals errata, bijlagen, aanbevelingen; deze documenten zijn voor het grootste gedeelte uit de collectie gefilterd. Deze documenten zijn opgeslagen in `./Rapporten/`.\n",
    "\n",
    "Omdat de meeste rapporten van de OVV zijn voorzien van een beveiliging tegen het kopieren van tekst, zijn alle rapporten eerst gedecrypt met een extern programma. De gedecrypte bestanden zijn te vinden in de map `./Rapporten decrypted/`. De onderliggende submappen geven de taal van het document aan: Nederlands `nl` of Engels `en`.\n",
    "\n",
    "Er is een handmatige selectie van rapporten gemaakt en geplaatst in `./Rapporten selectie/nl` met een validatie set in `./Rapporten selectie/validatie`.\n",
    "\n",
    "#### Mappenstructuur\n",
    "1. Rapporten\n",
    "    - en: bevat gedownloade Engelse rapporten.\n",
    "    - nl: bevat gedownloade Nederlandse rapporten.\n",
    "2. Rapporten decrypted\n",
    "    - en: bevat gedecrypte Engelse rapporten.\n",
    "    - nl: bevat gedecrypte Nederlandse rapporten.\n",
    "3. Rapporten selectie\n",
    "    - en: bevat Engelse rapporten.\n",
    "    - nl: bevat Nederlandse rapporten.\n",
    "    - validatie: bevat documenten ter validatie. Gebruiker moet zelf nl of en documenten als validatieset selecteren en plaatsen in deze map.\n",
    "4. Output\n",
    "    - logfile\n",
    "    - plots\n",
    "\n",
    "\n",
    "#### NLTK en overige pakketten\n",
    "Voor het minen maken we gebruik van de natural language toolkit ([NLTK](http://www.nltk.org/)), het textmining pakket ([textmining1.0](https://pypi.python.org/pypi/textmining/1.0)) en het boek _Natural Language Processing with Python_<sup id=\"a1\">[1](#f1)</sup>. Daarnaast maken we ook gebruik van pakketten voor lineaire algebra en matrix operaties, e.a. Deze pakketten importeren we eerst.\n",
    "\n",
    "<b id=\"f1\">1</b> Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \"O'Reilly Media, Inc.\". [â†©](#a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import nltk, textmining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text laden\n",
    "\n",
    "Als we gebruik willen maken van onze eigen corpus, database met text, dan moeten we de rapporten die gedownload zijn in pdf eerst converteren naar plain text (txt). Hiervoor gebruiken we `pdfminer`. Metadata halen we uit de pdf bestanden met het pakket `pyPdf` omdat deze sneller is.\n",
    "\n",
    "In de volgende code importeren we eerst alle pakketten die nodig zijn, daarna definieren we de bestandlocatie en creeeren we een functie om een bestand te kunnen lezen. In deze functie wordt elke pagina van het desbetreffende bestand geparsed en bij elkaar gevoegd.\n",
    "\n",
    "Om bij te houden welke acties er zijn uitgevoerd, welke settings zijn gebruikt en de datum en tijd van de run maken we ook nog een settings dict die zal worden geprint bij het uitvoeren van log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys\n",
    "import os\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from cStringIO import StringIO\n",
    "from pyPdf import PdfFileReader # extract pdf metadata\n",
    "import datetime # to set date and time\n",
    "\n",
    "# Define language\n",
    "lang = 'nl'\n",
    "\n",
    "# Define location of reports\n",
    "#FilePath = './Rapporten decrypted/' + lang + '/'\n",
    "FilePath = './Rapporten selectie/' + lang + '/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames = os.listdir(FilePath)\n",
    "\n",
    "# Make function to convert pdf to text\n",
    "def pdfparser(FilePath,FileName): \n",
    "    fp = file(FilePath+FileName, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    # Process each page contained in the document.\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    \n",
    "    # return text\n",
    "    return data\n",
    "\n",
    "# define settings dict, will be updated when running certain components\n",
    "now = datetime.datetime.now()\n",
    "run_settings = {\"title\": \"OVV Text Mining - Rapporten\", \"datetime\": now.strftime(\"%d-%m-%Y %H:%M\"), \n",
    "                \"text_stemmed\": False, \"stopwords_removed\": False, \"language\": lang, \"path\": FilePath,\n",
    "                \"dendogram\": None, \"kmeans\": None, \"kmeans_html\": None, \"timeline\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de functie om een bestand te lezen gedefineerd is kunnen we deze functie toe gaan passen op alle bestanden. We doen dit door over de bestanden heen te loopen.\n",
    "\n",
    "In de loop gebeurt het volgende: \n",
    "1. het bestand wordt gelezen\n",
    "2. de metadata wordt gelezen en opgeslagen in de dict `metadict`\n",
    "3. met het Regexp `re` pakket splitten we de tekst\n",
    "4. converteren de tekst naar lower case\n",
    "5. uitfilteren van lege indices\n",
    "6. uitfilteren van numerieke waarden\n",
    "7. als de overgebleven body niet `None` is dan voegen we de tokens toe aan dict `textdict`\n",
    "\n",
    "_**Let op**_ om geparallelliseerd bestanden te lezen en converteren, sla de volgende stappen over en ga naar [hier](#parallel). Als je systeem voorzien is van meerdere cores kan dit significant sneller zijn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regexp package to split\n",
    "import re\n",
    "\n",
    "# Prealloceer variabelen\n",
    "textdict = {}\n",
    "metadict = {}\n",
    "\n",
    "for i in np.arange(np.shape(FileNames)[0]): # use this line for all files\n",
    "#for i in np.arange(0,30): # only take first 30 files\n",
    "    print \"Processing \",i+1,'/',np.shape(FileNames)[0],': ', FileNames[i],'\\n'\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath,FileNames[i])\n",
    "        \n",
    "        # add to fulltextmat\n",
    "        #fulltextmat.append(data)\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath,FileNames[i]]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict[FileNames[i]] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        # filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        # filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "\n",
    "            # add to textbody array\n",
    "            #textbody = np.append(textbody,data)\n",
    "        \n",
    "            # also add data to dict and matrix for later use\n",
    "            textdict[FileNames[i]] = list(data)\n",
    "            #textmat.append(list(data))\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileNames[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geparallelliseerd bestanden lezen en converteren\n",
    " <a name=\"parallel\"></a>\n",
    " \n",
    "Om een groot gedeelte van de `for` loops sneller te maken, kunnen we de processen parallelliseren met `multiprocessing`. Dit pakket moeten we eerst importeren en dan definieren wat we ermee willen doen.\n",
    "\n",
    "Om het uitlezen van bestanden te kunnen faciliteren moeten we ook eerst een functie definieren om een bestand te lezen en cleanen, deze functie noemen we _processandtokenize_. Deze functie wordt parallel uitgevoerd en de output is een dict met daarin al onze bestandnamen en bijbehorende tokens.\n",
    "\n",
    "_Let op_: het runnen van onderstaande code is erg CPU intensief en kan ervoor zorgen dat je systeem langzaam wordt gedurende het processen van de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import re\n",
    "\n",
    "def processandtokenize(FileName, dictname, metadict): \n",
    "    \n",
    "    # print progress, var FileNames is taken from shared memory, not required as function input\n",
    "    print \"Processing: \",int(np.arange(0,len(FileNames))[np.array(FileNames) == FileName])+1,'/',len(FileNames), FileName\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath,FileName)\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath,FileName]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict[FileName] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        ## filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        ## filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "        # add data to dict\n",
    "        dictname[FileName] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileName\n",
    "        \n",
    "\n",
    "# initialize multi-core process\n",
    "manager = multiprocessing.Manager()\n",
    "textdict = manager.dict()\n",
    "metadict = manager.dict()\n",
    "\n",
    "# define job\n",
    "#job = [multiprocessing.Process(target=processandtokenize, args=(i, textdict, metadict)) for i in FileNames[0:30]] # first 30 files\n",
    "job = [multiprocessing.Process(target=processandtokenize, args=(i, textdict, metadict)) for i in FileNames] # for all files\n",
    "\n",
    "\n",
    "# start all jobs in the job list\n",
    "_ = [p.start() for p in job]\n",
    "_ = [p.join() for p in job]\n",
    "\n",
    "# convert the multiprocess textdict opject to normal dict\n",
    "textdict = dict(textdict)\n",
    "metadict = dict(metadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total number of tokens: ',np.shape(sum(list(textdict.values()),[]))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "## Text stemming\n",
    "\n",
    "Nu alle woorden uit de text gehaald zijn en in een `numpy` array staan kunnen we de woorden gaan _stemmen_. We gebruiken hiervoor de Porter stemmer voor het Engels en de Snowball stemmer in de NLTK toolkit voor het Nederlands.\n",
    "\n",
    "Let op: de huidige implementatie van de Snowball stemmer is verre van perfect voor de Nederlandse taal. Deze stap kan ook overgeslagen worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "print \"Supported Languages Snowball: \\n\"\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Supported Languages Porter: \\n\"\n",
    "print \"english\\n\"\n",
    "\n",
    "print \"Processing \", lang, \"stopwords\\n\"\n",
    "\n",
    "# define function to stem text\n",
    "def stemtext(textdict):\n",
    "    if lang == 'nl':\n",
    "        stemmer = SnowballStemmer(\"dutch\")\n",
    "    elif lang == 'en':\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "    for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "        text_stemmed = []\n",
    "        print i+1,'/',np.shape(textdict.keys())[0]\n",
    "        for j in np.arange(0,np.shape(textdict.values()[i])[0]):\n",
    "            text_stemmed.append(stemmer.stem(textdict.values()[i][j]))\n",
    "        textdict[textdict.keys()[i]] = text_stemmed\n",
    "        \n",
    "# stem text\n",
    "stemtext(textdict)\n",
    "\n",
    "# update settings struct\n",
    "run_settings['text_stemmed'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords verwijderen\n",
    "\n",
    "Om onze teksten te processen moeten we nog een cleansing stap uitvoeren: het verwijderen van de stopwords. Hiervoor gebruiken we de standaard NL/EN library in de NLTK toolkit. Om een overzicht te krijgen van de stopwords in het NLTK pakket, gebruik het commando in de tweede cell. Het is mogelijk om hier meer woorden aan toe te voegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the stopwords\n",
    "print stopwords.words('dutch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to remove stopwords\n",
    "def removestopwords(textdict):\n",
    "    for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "        print i+1,'/',np.shape(textdict.keys())[0]\n",
    "        if lang == 'nl':\n",
    "            text_nostopw = [w for w in textdict.values()[i] if not w in stopwords.words('dutch')]\n",
    "        elif lang == 'en':\n",
    "            text_nostopw = [w for w in textdict.values()[i] if not w in stopwords.words('english')]\n",
    "        textdict[textdict.keys()[i]] = text_nostopw\n",
    "        \n",
    "# remove stopwords\n",
    "removestopwords(textdict)\n",
    "    \n",
    "# update settings dict\n",
    "run_settings['stopwords_removed'] = True\n",
    "\n",
    "# trick to flatten matrix: sum(list(textdict.values()),[])\n",
    "count = Counter(sum(list(textdict.values()),[]))\n",
    "\n",
    "# print de meest voorkomende woorden na het verwijderen van de stopwords\n",
    "print count.most_common(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Jaccard Similarity\n",
    "\n",
    "Om verschillende documenten met elkaar te vergelijken maken we gebruik van de _Jaccard Similarity_: $J\\left(A,B\\right) = \\frac{|A\\bigcap B|}{|A\\bigcup B|}$. Dit is de lengte van de intersectie (overeenkomst) van de set tokens van tekst $A$ en tekst $B$ gedeeld door de union (hele collectie) van beide sets. Deze measure geeft de numerieke overeenkomst, op een schaal van 0-1, tussen twee documenten. Met andere woorden: de Jaccard Similarity is een fractie die aangeeft hoeveel van tekst $A$ en $B$ overeenkomen. Set $A$ en set $B$ hoeven niet per definitie de zelfde grootte of lengte te hebben.\n",
    "\n",
    "![Image of Yaktocat](http://4.bp.blogspot.com/-x1n8llkCjuc/UAyeWdyc3EI/AAAAAAAAAU8/ron4CjbDVDM/s1600/DATA+SETS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Jaccard similarity\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return float(len(intersection))/len(union)\n",
    "\n",
    "# calculate the Jaccard similarity of text 1 and text 2\n",
    "A = textdict.values()[0]\n",
    "B = textdict.values()[1]\n",
    "print jaccard_similarity(A,B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De Jaccard Similarity kan op veel verschillende manieren worden berekend. We kunnen de measure berekenen voor:\n",
    "1. de bag of words zonder rekening te houden met woordfrequentie, we gebruiken dan alleen de set met unieke woorden voor beide teksten\n",
    "2. eerst de stopwords verwijderen en dan de bag of words gebruiken zonder frequentie\n",
    "3. rekening houden met de woordfrequentie\n",
    "4. meer geavanceerde methodes zoals k-shingles of n-grams\n",
    "\n",
    "In deze implementatie is ervoor gekozen om 2. te gebruiken (afhankelijk van de eerder uitgevoerde stappen kan ook 1. gebruikt worden).\n",
    "\n",
    "Om de intersectie van beide teksten te bekijken, gebruik het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the intersection of both texts:\n",
    "set(A).intersection(set(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - _term frequency-inverse document frequency_\n",
    "\n",
    "Om het belang van verschillende woorden in een corpus numeriek uit te drukken berekenen we de TF-IDF. Dit doen we door gebruik te maken van de functie `feature_extraction` in de `sklearn` toolbox (onderdeel van `scikit`). \n",
    "\n",
    "De TF-IDF measure reflecteert hoe belangrijk een woord voor een document is door in eerste instantie te kijken hoe vaak een woord voor komt in een tekst. In het meest simpele geval gebruiken we hiervoor de woordfrequentie $f_{t,d}$. Ons algorimte maakt gebruik van de geaugmenteerde woordfrequentie $tf$:\n",
    "\n",
    "$\\text{tf}(t,d) = 0.5 + \\frac{0.5 ~\\times ~f(t,d)}{\\text{max}\\left\\{ f(w,d):~w~\\in ~d\\right\\}}$\n",
    "\n",
    "waarin de noemer de maximale woordfrequentie aangeeft.\n",
    "\n",
    "Omdat sommige woorden vaak in tekst aanwezig zijn maar eigenlijk weinig betekenis hebben maken we gebruik van de inverse document frequentie om de uitkomst van de woord frequentie te reguleren. De IDF is gedefinieerd als:\n",
    "\n",
    "$ \\text{idf}(t) = \\log \\frac{n_d}{df\\left( d,t \\right)} + 1$\n",
    "\n",
    "hierin is $n_d$ het totaal aantal documenten en $df\\left(t,d\\right)$ het aantal documenten dat term $t$ bevat.\n",
    "\n",
    "De TF-IDF krijgen we dan door:\n",
    "\n",
    "$ \\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times \\text{idf}(t)$\n",
    "\n",
    "De TF-IDF wordt berekend voor alle unieke termen $t$ in alle unieke documenten $d$. De resulterende matrix heeft dus de dimensie $\\left( t,d \\right)^{\\intercal}$ waarin element ($t_i,d_j)^{\\intercal}$ de numerieke relevantie geeft van term $i$ in document $j$.\n",
    "\n",
    "![Image of Yaktocat](http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402)\n",
    "\n",
    "#### Limitatie bag of words\n",
    "Omdat een simpel bag of words model geen rekening houdt met verkeerd gespelde woorden/typo's kunnen deze woorden als verschillend worden gezien door het algoritme. Om dit te voorkomen kunnen we gebruik maken van een collectie bigrams (n=2) of ngrams (n>2). Met een analyzer (in dit geval `char_wb`) en een gedefinieerde `ngram_range` kunnen we woorden dan herkennen aan $n$ features. Zo kunnen we voorkomen dat simpele fouten invloed hebben op het model.\n",
    "\n",
    "**Let op**: hoe meer ngrams features, hoe meer geheugen nodig is!\n",
    "\n",
    "#### Tokenizer definitie<a name=\"limitatie\"></a>\n",
    "Omdat `Tfidfvectorizer` alleen de volledige tekst als string accepteert moeten we eerst de tokens in `textdict` manipuleren -> we voegen de tokens samen tot een string en splitten die dan weer op de spatie tussen woorden. Dit is een workaround en genereert een kleine overhead.\n",
    "\n",
    "#### Overige parameters\n",
    "De standaardimplementatie van de vectorizer in de `sklearn` toolkit heeft een aantal parameters die aangepast kunnen worden, waaronder:\n",
    "1. __max_df__: dit is de maximale frequentie die een feature in documenten kan hebben om opgenomen te worden in de TF-IDF matrix. Features (woorden) met een hogere frequentie worden genegeerd. Als een term bijvoorbeeld in meer dan 80% van de documenten voorkomt heeft het waarschijnlijk weinig betekenis en/of is het lastig om aan de hand van deze feature documenten te onderscheiden.\n",
    "2. __min_idf__: minimale woordfrequentie, fractie die aangeeft in hoeveel documenten ten opzichte van de totale collectie een bepaald woord voorkomt, features met een lagere frequentie dan min_idf worden genegeerd.\n",
    "3. __ngram_range__: zie [limitatie bag of words en ngrams](#limitatie), de ngram_range (a,b) geeft aan dat alle mogelijke combinaties van woorden tussen lengte a en b gebruikt kunnen worden als feature als deze binnen de gestelde min_idf en max_idf vallen. Dit voorkomt dat verkeerde gespelde woorden, vervoegingen, etc. invloed uitoefenen op de resultaten.\n",
    "4. __use_idf__: gebruik inverse-document-frequentie\n",
    "5. __smooth_idf__: smooth idf door 1 bij de document frequentie op te tellen, dit voorkomt delen door nul en Inf errors.\n",
    "6. __sublinear_tf__: gebruik sublineaire schaling, vervang $tf$ door $1+\\log(tf)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# manipulate variable textdict to allow feeding into TfidfVectorizer function\n",
    "alltext = [' '.join(text) for text in textdict.values()]\n",
    "\n",
    "# define tokenizer\n",
    "# dummy function: we split concatenated tokens at space\n",
    "tfidf_tokenizer = lambda doc: [word for word in doc.split(' ')]\n",
    "\n",
    "# define numer of ngrams\n",
    "ngrams = (3,15)\n",
    "\n",
    "# define vectorizer -- using ngrams\n",
    "sklearn_tfidf = TfidfVectorizer(analyzer='char_wb',ngram_range=ngrams,norm='l2',min_df=0,max_df=0.8,\n",
    "                                use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "# define vectorizer -- unigram\n",
    "#sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0,max_df=0.8,use_idf=True,smooth_idf=False, \n",
    "#                                sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "tfidf_matrix = sklearn_tfidf.fit_transform(alltext)\n",
    "\n",
    "#print tfidf_matrix.toarray()[0].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print de lijst met termen. Let op, bij grote ngram_range is dit een zeer lange lijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = sklearn_tfidf.get_feature_names()\n",
    "\n",
    "if np.shape(tfidf_matrix)[1] < 50000:\n",
    "    print terms\n",
    "else:\n",
    "    print 'List too long to print'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans clustering\n",
    "\n",
    "Aan de hand van de TF-IDF matrix kunnen we de cosine similarity uitrekenen en de onderlinge (numerieke) afstand tussen verschillende text bestanden. Op basis van deze measure kan er dan met het KMeans algoritme een onderscheid gemaakt worden tussen verschillende clusters.\n",
    "\n",
    "Het aantal clusters moeten we zelf definieren. Dit is onderhevig aan trial en error: het zoeken van het optimale aantal clusters is geen triviaal proces en wordt vaak aangepakt met een brute force aanpak. Dit komt dus neer op $n$ keer het algoritme zijn clusters laten bepalen voor $n$ verschillende clustergrootten en achteraf bepalen met welke clustergrootte het beste resultaat wordt bereikt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "num_clusters = 10\n",
    "km = KMeans(n_clusters=num_clusters)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# print clusters\n",
    "for i in np.arange(0,num_clusters):\n",
    "    print \"Cluster\", i, \": \\n\"\n",
    "    print np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "clusters = km.labels_.tolist()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "# first check if file exists\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"         K-MEANS CLUSTERING         \\n\")\n",
    "    text_file.write(\"====================================\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        text_file.write(\"Cluster {}\\n\".format(i))\n",
    "        text_file.write(\"{}\".format(np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]))\n",
    "        text_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot K-means clusters\n",
    "Plot clusters by first applying multidimensional scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import MDS\n",
    "MDS()\n",
    "\n",
    "# dimension reduction -> reduce multi-dimensional field to 2 dimensions for plotting\n",
    "mds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n",
    "pos = mds.fit_transform(dist)\n",
    "xs, ys = pos[:,0], pos[:,1]\n",
    "\n",
    "# visualize clusters\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=textdict.keys())) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 10)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "cluster_names = ['Cluster '+str(w) for w in np.arange(np.min(clusters),np.max(clusters)+1,1)]\n",
    "cluster_colors = ['r','b','g','orange','k','brown','lime','cornsilk',\n",
    "                  'aqua','violet','grey','gold','darkred']\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[name], color=cluster_colors[name], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'x',          # changes apply to the x-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        bottom='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelbottom='off')\n",
    "    ax.tick_params(\\\n",
    "        axis= 'y',         # changes apply to the y-axis\n",
    "        which='both',      # both major and minor ticks are affected\n",
    "        left='off',      # ticks along the bottom edge are off\n",
    "        top='off',         # ticks along the top edge are off\n",
    "        labelleft='off')\n",
    "    \n",
    "ax.legend(numpoints=1)  #show legend with only 1 point\n",
    "\n",
    "#add label in x,y position with the label as the film title\n",
    "for i in range(len(df)):\n",
    "    ax.text(df.iloc[i]['x'], df.iloc[i]['y'], df.iloc[i]['title'], size=8)  \n",
    "\n",
    "#uncomment the below to save the plot if need be\n",
    "plt.savefig('./Output/Rapporten-clusters_plot ' + run_settings['datetime'] + '.pdf')\n",
    "run_settings['kmeans'] = './Output/Rapporten-clusters_plot ' + run_settings['datetime'] + '.pdf'\n",
    "plt.show() #show the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import mpld3\n",
    "#define custom toolbar location\n",
    "class TopToolbar(mpld3.plugins.PluginBase):\n",
    "    \"\"\"Plugin for moving toolbar to top of figure\"\"\"\n",
    "\n",
    "    JAVASCRIPT = \"\"\"\n",
    "    mpld3.register_plugin(\"toptoolbar\", TopToolbar);\n",
    "    TopToolbar.prototype = Object.create(mpld3.Plugin.prototype);\n",
    "    TopToolbar.prototype.constructor = TopToolbar;\n",
    "    function TopToolbar(fig, props){\n",
    "        mpld3.Plugin.call(this, fig, props);\n",
    "    };\n",
    "\n",
    "    TopToolbar.prototype.draw = function(){\n",
    "      // the toolbar svg doesn't exist\n",
    "      // yet, so first draw it\n",
    "      this.fig.toolbar.draw();\n",
    "\n",
    "      // then change the y position to be\n",
    "      // at the top of the figure\n",
    "      this.fig.toolbar.toolbar.attr(\"x\", 150);\n",
    "      this.fig.toolbar.toolbar.attr(\"y\", 400);\n",
    "\n",
    "      // then remove the draw function,\n",
    "      // so that it is not called again\n",
    "      this.fig.toolbar.draw = function() {}\n",
    "    }\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.dict_ = {\"type\": \"toptoolbar\"}\n",
    "        \n",
    "#define custom css to format the font and to remove the axis labeling\n",
    "css = \"\"\"\n",
    "text.mpld3-text, div.mpld3-tooltip {\n",
    "  font-family:Arial, Helvetica, sans-serif;\n",
    "}\n",
    "\n",
    "g.mpld3-xaxis, g.mpld3-yaxis {\n",
    "display: none; }\n",
    "\n",
    "svg.mpld3-figure {\n",
    "margin-left: -100px;}\n",
    "\"\"\"\n",
    "\n",
    "# Plot \n",
    "fig, ax = plt.subplots(figsize=(22,12)) #set plot size\n",
    "ax.margins(0.00) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "#iterate through groups to layer the plot\n",
    "#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color/label\n",
    "for name, group in groups:\n",
    "    points = ax.plot(group.x, group.y, marker='o', linestyle='', ms=18, \n",
    "                     label=cluster_names[name], mec='none', \n",
    "                     color=cluster_colors[name])\n",
    "    ax.set_aspect('auto')\n",
    "    labels = [i for i in group.title]\n",
    "    \n",
    "    #set tooltip using points, labels and the already defined 'css'\n",
    "    tooltip = mpld3.plugins.PointHTMLTooltip(points[0], labels,\n",
    "                                       voffset=10, hoffset=10, css=css)\n",
    "    #connect tooltip to fig\n",
    "    mpld3.plugins.connect(fig, tooltip, TopToolbar())    \n",
    "    \n",
    "    #set tick marks as blank\n",
    "    ax.axes.get_xaxis().set_ticks([])\n",
    "    ax.axes.get_yaxis().set_ticks([])\n",
    "    \n",
    "    #set axis as blank\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "    \n",
    "ax.legend(numpoints=1) #show legend with only one dot\n",
    "\n",
    "print 'Open de bestand vanuit de Output folder'\n",
    "\n",
    "mpld3.display() #show the plot\n",
    "\n",
    "#uncomment the below to export to html\n",
    "mpld3.save_html(fig, './Output/Rapporten-clusters ' + run_settings['datetime'] + '.html')\n",
    "run_settings['kmeans_html'] = './Output/Rapporten-clusters ' + run_settings['datetime'] + '.html'\n",
    "#html = mpld3.fig_to_html(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical document clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "linkage_matrix = ward(dist)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15./60*len(textdict.keys())))\n",
    "\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=textdict.keys());\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "plt.savefig('./Output/Rapport-dendogram_plot ' + run_settings['datetime'] + '.pdf')\n",
    "run_settings['dendogram'] = './Output/Rapport-dendogram_plot ' + run_settings['datetime'] + '.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Topic Modeling\n",
    "\n",
    "Om meer van de verborgen structuur in de tekstdocumenten te vinden kunnen we gebruik maken van _Latend Dirichlet Allocation_. LDA is een propabilistisch topic model die aanneemt dat documenten een mix zijn van verschillende onderwerpen (topics) en dat elk woord in het document bijdraagt aan dit onderwerp (en zo komt het onderwerp tot stand).\n",
    "\n",
    "![Image of Yaktocat](http://mengjunxie.github.io/ae-lda/img/IntroToLDA.png)\n",
    "\n",
    "Om LDA te gebruiken moeten we eerst een model trainen met de data (teksten) die we hebben.\n",
    "\n",
    "#### LDA model parameters\n",
    "Voor het trainen van een LDA model gebruiken we de standaard implementatie van `gensim`. In dit model zijn een aantal aan te passen parameters:\n",
    "\n",
    "1. __no_below__: houd tokens die minstens in dit aantal documenten aanwezig zijn.\n",
    "2. __no_above__: verwijder tokens die in dit aantal (fractie van totaal) documenten aanwezig zijn.\n",
    "3. __num_topics__: aantal te identificeren topic clusters. Dit is een van de belangrijkste parameters.\n",
    "4. __update_every__: aantal chunks die verwerkt worden voordat de stap wordt gemaakt van E naar M (EM: Expectation Maximization).\n",
    "5. __chunk_size__: aantal documenten die tegelijkertijd in geheugen geladen wordt.\n",
    "\n",
    "Voor meer info: https://miningthedetails.com/blog/python/lda/GensimLDA/\n",
    "\n",
    "Eerst importeren we de benodigde toolkits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken nu een dictionary van alle tokens in onze `textdict` variabele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(textdict.values())\n",
    "\n",
    "# remove extremes\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "# convert dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in textdict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# train the LDA model - TAKES A LONG TIME!\n",
    "%time lda = models.LdaModel(corpus, num_topics=10,id2word = dictionary,update_every=1,chunksize=100000,passes=50)\n",
    "\n",
    "# save lda model\n",
    "lda.save('./Output/lda '+ run_settings['datetime'] +'.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat de eerste 20 woorden zien van elk topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=30)\n",
    "\n",
    "# show topics\n",
    "topics_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"  LATENT DIRICHLET ALLOCATION model \\n\")\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"Training set feature scores\\n\\n\")\n",
    "    for i in np.arange(0,len(topics_matrix)):\n",
    "        text_file.write(\"Cluster {}\\n\".format(topics_matrix[i][0]))\n",
    "        text_file.write(str(topics_matrix[i][1]))\n",
    "        text_file.write(\"\\n\\n\")\n",
    "    text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document classificeren met LDA\n",
    "\n",
    "Om nu nieuwe documenten, die niet gebruikt zijn bij het trainen van bovenstaand LDA model (en dus niet deel uitmaken van de standaard corpus), te classificeren met LDA, laden we eerst de desbetreffende documenten en tokenizen deze met de eerder gedefinieerde `processandtokenize` functie.\n",
    "\n",
    "Daarna kunnen we de documenten invoeren in het LDA model. Hier komt vervolgens een score uit. In het geval een document scores behaalt binnen meerdere LDA clusters dan worden al deze scores gereturned. Als stopwords zijn verwijderd en de text 'gestemd' is voor de trainingsset doen we dat ook voor de validatieset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilePath_val = './Rapporten selectie/validatie/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames_val = os.listdir(FilePath_val)\n",
    "\n",
    "# Preallocate variables\n",
    "textdict_val = {}\n",
    "metadict_val = {}\n",
    "\n",
    "for i in np.arange(np.shape(FileNames_val)[0]): # use this line for all files\n",
    "    print \"Processing \",i+1,'/',np.shape(FileNames_val)[0],': ', FileNames_val[i],'\\n'\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath_val,FileNames_val[i])\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath_val,FileNames_val[i]]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict_val[FileNames_val[i]] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        # filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        # filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "            # add data to dict\n",
    "            textdict_val[FileNames_val[i]] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileNames_val[i]\n",
    "        \n",
    "# remove stopwords and stem text\n",
    "if run_settings['stopwords_removed'] == True:\n",
    "    removestopwords(textdict_val)\n",
    "if run_settings['text_stemmed'] == True:\n",
    "    stemtext(textdict_val)\n",
    "        \n",
    "# Create validation dictionary and corpus\n",
    "dictionary_val = corpora.Dictionary(textdict_val.values())\n",
    "corpus_val = [dictionary_val.doc2bow(text) for text in textdict_val.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify texts and print results\n",
    "for i in np.arange(0,np.shape(FileNames_val)[0]):\n",
    "    modscore = lda[corpus_val[i]]\n",
    "    print FileNames_val[i], ':'\n",
    "    clusters = np.array([])\n",
    "    scores = np.array([])\n",
    "    for j in np.arange(0,np.shape(modscore)[0]):\n",
    "        clusters = np.append(clusters, modscore[j][0])\n",
    "        scores = np.append(scores, modscore[j][1])\n",
    "    order = np.flip(np.argsort(scores), 0)\n",
    "    print 'LDA cluster      score'\n",
    "    for j in order:\n",
    "        print int(clusters[j]), '              ', scores[j]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"     LATENT DIRICHLET ALLOCATION    \\n\")\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"Validation set classification cluster scores\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        modscore = lda[corpus_val[i]]\n",
    "        clusters = np.array([])\n",
    "        scores = np.array([])\n",
    "        for j in np.arange(0,np.shape(modscore)[0]):\n",
    "            clusters = np.append(clusters, modscore[j][0])\n",
    "            scores = np.append(scores, modscore[j][1])\n",
    "        order = np.flip(np.argsort(scores), 0)\n",
    "        text_file.write(FileNames_val[i]+'\\n')\n",
    "        text_file.write(\"LDA cluster      score\\n\")\n",
    "        for j in order:\n",
    "            text_file.write(str(int(clusters[j])) + '                ' + str(scores[j]) + \"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "    text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finaliseer logbestand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to add text to beginning of file\n",
    "def line_prepender(filename, line):\n",
    "    with open(filename, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line.rstrip('\\r\\n') + '\\n' + content)\n",
    "        \n",
    "# add title and settings info\n",
    "head_settings = \"\"\n",
    "for k,v in run_settings.items():\n",
    "    head_settings = head_settings + str(k) + ': ' + str(v) + \"\\n\"\n",
    "header = run_settings['title'] + \"\\n\\n\" + \"====================================\\n\" + \\\n",
    "         \"             RUN SETTINGS             \\n\" + \"====================================\\n\\n\" + \\\n",
    "         head_settings + \"\\n\\n\"\n",
    "\n",
    "line_prepender(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = np.array([])\n",
    "\n",
    "# convert metadict date field to date array\n",
    "for i in np.arange(0,len(metadict)):\n",
    "    year = metadict[metadict.keys()[i]]['/CreationDate'][2:6]\n",
    "    month = metadict[metadict.keys()[i]]['/CreationDate'][6:8]\n",
    "    day = metadict[metadict.keys()[i]]['/CreationDate'][8:10]\n",
    "\n",
    "    # convert to python datetime object\n",
    "    date = np.append(date,datetime.date(int(year), int(month), int(day)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as pltdates\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# set plot properties\n",
    "plt.title(\"Tijdlijn rapporten\", fontsize=22)\n",
    "fig.set_size_inches(20,20./60*len(textdict.keys()))\n",
    "fig.gca().spines['left'].set_visible(False)\n",
    "fig.gca().spines['right'].set_visible(False)\n",
    "fig.gca().spines['top'].set_visible(False)\n",
    "fig.gca().get_yaxis().set_ticks([]) # remove y ticks\n",
    "fig.gca().axes.yaxis.set_ticklabels([]) # remove y labels\n",
    "plt.grid(True,which='major',axis='x')\n",
    "\n",
    "# plot data\n",
    "xplot = np.arange(0,np.shape(textdict.keys())[0])\n",
    "yind = np.argsort(date) # sort date for plotting\n",
    "plt.scatter(date[yind], xplot, c=(75./256,151./256,182./256),s=100) \n",
    "for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "    plt.text(date[yind[i]]+datetime.timedelta(60), i, metadict.keys()[yind[i]], fontsize=8)\n",
    "    \n",
    "plt.xticks(np.arange(datetime.date(min(date).year,1,1),datetime.date(max(date).year+1,1,1),365),fontsize=16,rotation=70)\n",
    "    \n",
    "# make sure text fits in axis\n",
    "plt.xlim(fig.gca().get_xlim()[0],fig.gca().get_xlim()[1]*1.003)\n",
    "\n",
    "# show figure and save figure to working directory\n",
    "plt.savefig('./Output/Rapport-datum_plot.pdf', dpi=80, orientation='landscape')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEE: toekomstige implementaties\n",
    "\n",
    "Tekst met keywords in pdf markeren, zie voorbeelden om locatie van het woord te extracten en de tekst in het PDF bestand te highlighten:\n",
    "\n",
    "- https://stackoverflow.com/questions/7605577/read-highlight-save-pdf-programatically\n",
    "- https://stackoverflow.com/questions/22898145/how-to-extract-text-and-text-coordinates-from-a-pdf-file/\n",
    "\n",
    "\n",
    "Gebruik PCA of t-SNE bij clusteren\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
