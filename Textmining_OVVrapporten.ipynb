{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining met Python - OVV Rapporten\n",
    "\n",
    "|   |   |\n",
    "|------|------|\n",
    "|__Auteur__ | Marlon van den Hoek  |\n",
    "| __E-mail__  | <m.vandenhoek@onderzoeksraad.nl>  |\n",
    "| __Onderwerpen__ | text mining, cleansing, tokenizing, stemming, stopwords, TF-IDF, Jaccard similarity, KMeans, hierarchical document clustering, LDA, sentiment analysis |\n",
    "| __Afhankelijkheden__ | `nltk`, `textmining`, `pdfminer`, `re`, `multiprocessing`, `sklearn`, `pandas`, `wordcloud`, `gensim`, `plotly` (alleen voor plotting) |\n",
    "|__Datum__ | 14-02-2018 |\n",
    "| __Versie__ | `0.7` |\n",
    "\n",
    "_Opmerking 1_: Om dit Jupyter Notebook correct te kunnen runnen moeten eerst de juiste pakketten (zie afhankelijkheden) geinstalleerd worden. Dit kan bijvoorbeeld met `pip`. Zie hiervoor de eerste cell hier onder.\n",
    "\n",
    "_Opmerking 2_: Dit notebook is standaard beperkt tot 30 documenten. Om alle documenten te analyseren, verwijder de limiet van 30 bij het laden: verander `arange` bij de for loop, `job` bij de multicore loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install the dependencies, run the following code -- run only once!!\n",
    "! pip install nltk\n",
    "! pip install textmining\n",
    "! pip install pdfminer\n",
    "! pip install multiprocessing\n",
    "! pip install sklearn\n",
    "! pip install gensim\n",
    "! pip install plotly\n",
    "! pip install wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introductie\n",
    "\n",
    "In dit Jupyter notebook gaan we aan de slag met text mining op de (openbare) rapporten van de Onderzoeksraad. Met een ander script (scraper) zijn meer dan 600 rapporten van de OVV website gedownload. Daarin is rekening gehouden met andere documenten zoals errata, bijlagen, aanbevelingen; deze documenten zijn voor het grootste gedeelte uit de collectie gefilterd. Deze documenten zijn opgeslagen in `./Rapporten/`.\n",
    "\n",
    "Omdat de meeste rapporten van de OVV zijn voorzien van een beveiliging tegen het kopieren van tekst, zijn alle rapporten eerst gedecrypt met een extern programma. De gedecrypte bestanden zijn te vinden in de map `./Rapporten decrypted/`. De onderliggende submappen geven de taal van het document aan: Nederlands `nl` of Engels `en`.\n",
    "\n",
    "Er is een handmatige selectie van rapporten gemaakt en geplaatst in `./Rapporten selectie/nl` met een validatie set in `./Rapporten selectie/validatie`.\n",
    "\n",
    "#### Mappenstructuur\n",
    "1. Rapporten\n",
    "    - en: bevat gedownloade Engelse rapporten.\n",
    "    - nl: bevat gedownloade Nederlandse rapporten.\n",
    "2. Rapporten decrypted\n",
    "    - en: bevat gedecrypte Engelse rapporten.\n",
    "    - nl: bevat gedecrypte Nederlandse rapporten.\n",
    "3. Rapporten selectie\n",
    "    - en: bevat Engelse rapporten.\n",
    "    - nl: bevat Nederlandse rapporten.\n",
    "    - validatie: bevat documenten ter validatie. Gebruiker moet zelf nl of en documenten als validatieset selecteren en plaatsen in deze map.\n",
    "4. Output\n",
    "    - logfile\n",
    "    - plots\n",
    "\n",
    "\n",
    "#### NLTK en overige pakketten\n",
    "Voor het minen maken we gebruik van de natural language toolkit ([NLTK](http://www.nltk.org/)), het textmining pakket ([textmining1.0](https://pypi.python.org/pypi/textmining/1.0)) en het boek _Natural Language Processing with Python_<sup id=\"a1\">[1](#f1)</sup>. Daarnaast maken we ook gebruik van pakketten voor lineaire algebra en matrix operaties, e.a. Deze pakketten importeren we eerst.\n",
    "\n",
    "<b id=\"f1\">1</b> Bird, S., Klein, E., & Loper, E. (2009). Natural language processing with Python: analyzing text with the natural language toolkit. \"O'Reilly Media, Inc.\". [â†©](#a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dependencies\n",
    "import numpy as np\n",
    "import nltk, textmining\n",
    "import sys\n",
    "import os\n",
    "import datetime # to set date and time\n",
    "from cStringIO import StringIO\n",
    "import scipy as sc\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorige sessie laden\n",
    "\n",
    "Indien we een corpus hebben opgebouwd in een vorige sessie kunnen we deze hier laden. In het bestand moeten de volgende variabelen zijn opgeslagen:\n",
    "1. lang\n",
    "2. FilePath\n",
    "3. FileNames\n",
    "4. run_settings\n",
    "5. textdict\n",
    "6. metadict\n",
    "\n",
    "Als je van plan bent om de teksten opnieuw te processen en tokenizen, sla deze stap over!\n",
    "\n",
    "Laad de variabelen als volgt:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang = str(np.load('./Vars/lang.npy'))\n",
    "FilePath = str(np.load('./Vars/filepath.npy'))\n",
    "FileNames = np.load('./Vars/filenames.npy')\n",
    "run_settings = eval(str((np.load('./Vars/run_settings.npy'))))\n",
    "textdict = eval(str((np.load('./Vars/textdict.npy'))))\n",
    "metadict = eval(str((np.load('./Vars/metadict.npy'))))\n",
    "\n",
    "# set date to current date\n",
    "#now = datetime.datetime.now()\n",
    "#run_settings['datetime'] = now.strftime(\"%d-%m-%Y %H:%M\")\n",
    "\n",
    "print(run_settings)\n",
    "print('Total number of tokens: ',np.shape(sum(list(textdict.values()),[]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nieuwe text laden\n",
    "\n",
    "Als we gebruik willen maken van onze eigen corpus, database met text, dan moeten we de rapporten die gedownload zijn in pdf eerst converteren naar plain text (txt). Hiervoor gebruiken we `pdfminer`. Metadata halen we uit de pdf bestanden met het pakket `pyPdf` omdat deze sneller is.\n",
    "\n",
    "In de volgende code importeren we eerst alle pakketten die nodig zijn, daarna definieren we de bestandlocatie en creeeren we een functie om een bestand te kunnen lezen. In deze functie wordt elke pagina van het desbetreffende bestand geparsed en bij elkaar gevoegd.\n",
    "\n",
    "Om bij te houden welke acties er zijn uitgevoerd, welke settings zijn gebruikt en de datum en tijd van de run maken we ook nog een settings dict die zal worden geprint bij het uitvoeren van log files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from pdfminer.converter import XMLConverter, HTMLConverter, TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pyPdf import PdfFileReader # extract pdf metadata\n",
    "\n",
    "# Define language\n",
    "lang = 'nl'\n",
    "\n",
    "# Define location of reports\n",
    "#FilePath = './Rapporten decrypted/' + lang + '/'\n",
    "FilePath = './Rapporten selectie/' + lang + '/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames = os.listdir(FilePath)\n",
    "\n",
    "# Make function to convert pdf to text\n",
    "def pdfparser(FilePath,FileName): \n",
    "    fp = file(FilePath+FileName, 'rb')\n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    retstr = StringIO()\n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams=laparams)\n",
    "    \n",
    "    # Create a PDF interpreter object.\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    \n",
    "    # Process each page contained in the document.\n",
    "    for page in PDFPage.get_pages(fp):\n",
    "        interpreter.process_page(page)\n",
    "        data =  retstr.getvalue()\n",
    "    \n",
    "    # return text\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define settings dict, will be updated when running certain components\n",
    "now = datetime.datetime.now()\n",
    "run_settings = {\"title\": \"OVV Text Mining - Rapporten\", \"datetime\": now.strftime(\"%d-%m-%Y %H:%M\"), \n",
    "                \"text_stemmed\": False, \"stopwords_removed\": False, \"language\": lang, \"path\": FilePath,\n",
    "                \"dendogram\": None, \"kmeans\": None, \"kmeans_html\": None, \"timeline\": None, \"kmeans_pca\": None,\n",
    "                \"kmeans_pca_html\": None, \"kmeans_tsne\": None, \"kmeans_tsne_html\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In de volgende cell definieren we een functie om te text te processen en tokenizen. Deze functie is van belang voor een aantal routines in de rest van dit document, het is daarom belangrijk deze cell te runnen.\n",
    "<a name=\"processtokenize\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processandtokenize(FileName, dictname, metadict): \n",
    "    \n",
    "    # print progress, var FileNames is taken from shared memory, not required as function input\n",
    "    print \"Processing: \",int(np.arange(0,len(FileNames))[np.array(FileNames) == FileName])+1,'/',len(FileNames), FileName\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath,FileName)\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath,FileName]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict[FileName] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        ## filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        ## filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "        # add data to dict\n",
    "        dictname[FileName] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nu de functie om een bestand te lezen gedefineerd is kunnen we deze functie toe gaan passen op alle bestanden. We doen dit door over de bestanden heen te loopen.\n",
    "\n",
    "In de loop gebeurt het volgende: \n",
    "1. het bestand wordt gelezen\n",
    "2. de metadata wordt gelezen en opgeslagen in de dict `metadict`\n",
    "3. met het Regexp `re` pakket splitten we de tekst\n",
    "4. converteren de tekst naar lower case\n",
    "5. uitfilteren van lege indices\n",
    "6. uitfilteren van numerieke waarden\n",
    "7. als de overgebleven body niet `None` is dan voegen we de tokens toe aan dict `textdict`\n",
    "\n",
    "_**Let op**_ om geparallelliseerd bestanden te lezen en converteren, sla de volgende stappen over en ga naar [hier](#parallel). Als je systeem voorzien is van meerdere cores kan dit significant sneller zijn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prealloceer variabelen\n",
    "textdict = {}\n",
    "metadict = {}\n",
    "\n",
    "for FileName in FileNames:\n",
    "    processandtokenize(FileName, textdict, metadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geparallelliseerd bestanden lezen en converteren\n",
    " <a name=\"parallel\"></a>\n",
    " \n",
    "Om een groot gedeelte van de `for` loops sneller te maken, kunnen we de processen parallelliseren met `multiprocessing`. Dit pakket moeten we eerst importeren en dan definieren wat we ermee willen doen.\n",
    "\n",
    "Om het uitlezen van bestanden te kunnen faciliteren maken we gebruik van de functie _processandtokenize_. Deze functie wordt parallel uitgevoerd en de output is een dict met daarin al onze bestandnamen en bijbehorende tokens.\n",
    "\n",
    "_Let op_: het runnen van onderstaande code is erg CPU intensief en kan ervoor zorgen dat je systeem langzaam wordt gedurende het processen van de data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing      \n",
    "\n",
    "# initialize multi-core process\n",
    "#manager = multiprocessing.Manager()\n",
    "#textdict = manager.dict()\n",
    "#metadict = manager.dict()\n",
    "\n",
    "# define job\n",
    "#job = [multiprocessing.Process(target=processandtokenize, args=(i, textdict, metadict)) for i in FileNames] # for all files\n",
    "\n",
    "\n",
    "# start all jobs in the job list\n",
    "#_ = [p.start() for p in job]\n",
    "#_ = [p.join() for p in job]\n",
    "\n",
    "# convert the multiprocess textdict opject to normal dict\n",
    "#textdict = dict(textdict)\n",
    "#metadict = dict(metadict)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # initialize manager and shared memory vars\n",
    "    manager = multiprocessing.Manager()\n",
    "    num_parprocess = multiprocessing.cpu_count()-1\n",
    "    textdict = manager.dict()\n",
    "    metadict = manager.dict()\n",
    "    \n",
    "    # initialize pool and add processes\n",
    "    pool = multiprocessing.Pool(processes=num_parprocess)\n",
    "    for i in FileNames:\n",
    "        pool.apply_async(processandtokenize, args=(i, textdict, metadict))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "# convert shared vars to normal dict\n",
    "textdict = dict(textdict)\n",
    "metadict = dict(metadict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Total number of tokens: ',np.shape(sum(list(textdict.values()),[]))[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords verwijderen\n",
    "\n",
    "Om onze teksten te processen moeten we nog een cleansing stap uitvoeren: het verwijderen van de stopwords. Hiervoor gebruiken we de standaard NL/EN library in de NLTK toolkit. Om een overzicht te krijgen van de stopwords in het NLTK pakket, gebruik het commando in de tweede cell. Het is mogelijk om hier meer woorden aan toe te voegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import ngrams\n",
    "from collections import Counter\n",
    "\n",
    "useownstopwords = True\n",
    "stopwords_path = './Stopwords/'\n",
    "\n",
    "if useownstopwords:\n",
    "    sw_dutch = pd.Series.from_csv(stopwords_path + 'stopwords_nl.csv', index_col=False).tolist()\n",
    "    sw_english = pd.Series.from_csv(stopwords_path + 'stopwords_en.csv', index_col=False).tolist()\n",
    "else:\n",
    "    sw_dutch = stopwords.words('dutch')\n",
    "    sw_english = stopwords.words('english')\n",
    "\n",
    "# define function to remove stopwords\n",
    "def removestopwords(textdict):\n",
    "    for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "        print i+1,'/',np.shape(textdict.keys())[0]\n",
    "        if lang == 'nl':\n",
    "            text_nostopw = [w for w in textdict.values()[i] if not w in sw_dutch and\n",
    "                                                               not w in sw_english] # haal voor zekerheid ook engelse eruit\n",
    "        elif lang == 'en':\n",
    "            text_nostopw = [w for w in textdict.values()[i] if not w in sw_english]\n",
    "        textdict[textdict.keys()[i]] = text_nostopw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "removestopwords(textdict)\n",
    "    \n",
    "# update settings dict\n",
    "run_settings['stopwords_removed'] = True\n",
    "\n",
    "# trick to flatten matrix: sum(list(textdict.values()),[])\n",
    "count = Counter(sum(list(textdict.values()),[]))\n",
    "\n",
    "# print de meest voorkomende woorden na het verwijderen van de stopwords\n",
    "print(count.most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud visualisatie\n",
    "\n",
    "Voordat we verdergaan met het stemmen van de woorden, kunnen we een visualisatie maken van de top 300 woorden met de volgende code:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "count = Counter(sum(list(textdict.values()),[]))\n",
    "words = count.most_common(400)\n",
    "wcloud = []\n",
    "\n",
    "for word, num in words:\n",
    "    wcloud = wcloud + [word] * num\n",
    "\n",
    "wcloud = \" \".join(wcloud)\n",
    "\n",
    "wordcloud = WordCloud(background_color='white',stopwords=None,\n",
    "                      max_font_size=70,scale=3,random_state=1, \n",
    "                      collocations=False,width=1200, \n",
    "                      height=720, max_words=400).generate(wcloud)\n",
    "\n",
    "plt.figure(figsize=(40, 24))\n",
    "plt.axis('off')\n",
    "plt.imshow(wordcloud)\n",
    "plt.savefig('./Output/Rapporten - wordcloud.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text stemming\n",
    "\n",
    "Nu alle woorden uit de text gehaald zijn en in een `numpy` array staan kunnen we de woorden gaan _stemmen_. We gebruiken hiervoor de Porter stemmer voor het Engels en de Snowball stemmer in de NLTK toolkit voor het Nederlands.\n",
    "\n",
    "Let op: de huidige implementatie van de Snowball stemmer is verre van perfect voor de Nederlandse taal. Deze stap kan ook overgeslagen worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "print \"Supported Languages Snowball: \\n\"\n",
    "print(\" \".join(SnowballStemmer.languages))\n",
    "print \"\\n\"\n",
    "\n",
    "print \"Supported Languages Porter: \\n\"\n",
    "print \"english\\n\"\n",
    "\n",
    "# define function to stem text\n",
    "def stemtext(textdict):\n",
    "    if lang == 'nl':\n",
    "        stemmer = SnowballStemmer(\"dutch\")\n",
    "    elif lang == 'en':\n",
    "        stemmer = PorterStemmer()\n",
    "\n",
    "    for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "        text_stemmed = []\n",
    "        print i+1,'/',np.shape(textdict.keys())[0]\n",
    "        for j in np.arange(0,np.shape(textdict.values()[i])[0]):\n",
    "            text_stemmed.append(stemmer.stem(textdict.values()[i][j]))\n",
    "        textdict[textdict.keys()[i]] = text_stemmed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem text\n",
    "print \"Stemming text: \\n\"\n",
    "stemtext(textdict)\n",
    "\n",
    "# update settings struct\n",
    "run_settings['text_stemmed'] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Jaccard Similarity\n",
    "\n",
    "Om verschillende documenten met elkaar te vergelijken maken we gebruik van de _Jaccard Similarity_: $J\\left(A,B\\right) = \\frac{|A\\bigcap B|}{|A\\bigcup B|}$. Dit is de lengte van de intersectie (overeenkomst) van de set tokens van tekst $A$ en tekst $B$ gedeeld door de union (hele collectie) van beide sets. Deze measure geeft de numerieke overeenkomst, op een schaal van 0-1, tussen twee documenten. Met andere woorden: de Jaccard Similarity is een fractie die aangeeft hoeveel van tekst $A$ en $B$ overeenkomen. Set $A$ en set $B$ hoeven niet per definitie de zelfde grootte of lengte te hebben.\n",
    "\n",
    "![Image](http://4.bp.blogspot.com/-x1n8llkCjuc/UAyeWdyc3EI/AAAAAAAAAU8/ron4CjbDVDM/s1600/DATA+SETS.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define Jaccard similarity\n",
    "def jaccard_similarity(query, document):\n",
    "    intersection = set(query).intersection(set(document))\n",
    "    union = set(query).union(set(document))\n",
    "    return float(len(intersection))/len(union)\n",
    "\n",
    "# calculate the Jaccard similarity of text 1 and text 2\n",
    "A = textdict.values()[0]\n",
    "B = textdict.values()[1]\n",
    "print jaccard_similarity(A,B)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De Jaccard Similarity kan op veel verschillende manieren worden berekend. We kunnen de measure berekenen voor:\n",
    "1. de bag of words zonder rekening te houden met woordfrequentie, we gebruiken dan alleen de set met unieke woorden voor beide teksten\n",
    "2. eerst de stopwords verwijderen en dan de bag of words gebruiken zonder frequentie\n",
    "3. rekening houden met de woordfrequentie\n",
    "4. meer geavanceerde methodes zoals k-shingles of n-grams\n",
    "\n",
    "In deze implementatie is ervoor gekozen om 2. te gebruiken (afhankelijk van de eerder uitgevoerde stappen kan ook 1. gebruikt worden).\n",
    "\n",
    "Om de intersectie van beide teksten te bekijken, gebruik het volgende commando:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to check the intersection of both texts:\n",
    "set(A).intersection(set(B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF - _term frequency-inverse document frequency_\n",
    "\n",
    "Om het belang van verschillende woorden in een corpus numeriek uit te drukken berekenen we de TF-IDF. Dit doen we door gebruik te maken van de functie `feature_extraction` in de `sklearn` toolbox (onderdeel van `scikit`). \n",
    "\n",
    "De TF-IDF measure reflecteert hoe belangrijk een woord voor een document is door in eerste instantie te kijken hoe vaak een woord voor komt in een tekst. In het meest simpele geval gebruiken we hiervoor de woordfrequentie $f_{t,d}$. Ons algorimte maakt gebruik van de geaugmenteerde woordfrequentie $tf$:\n",
    "\n",
    "$\\text{tf}(t,d) = 0.5 + \\frac{0.5 ~\\times ~f(t,d)}{\\text{max}\\left\\{ f(w,d):~w~\\in ~d\\right\\}}$\n",
    "\n",
    "waarin de noemer de maximale woordfrequentie aangeeft.\n",
    "\n",
    "Omdat sommige woorden vaak in tekst aanwezig zijn maar eigenlijk weinig betekenis hebben maken we gebruik van de inverse document frequentie om de uitkomst van de woord frequentie te reguleren. De IDF is gedefinieerd als:\n",
    "\n",
    "$ \\text{idf}(t) = \\log \\frac{n_d}{df\\left( d,t \\right)} + 1$\n",
    "\n",
    "hierin is $n_d$ het totaal aantal documenten en $df\\left(t,d\\right)$ het aantal documenten dat term $t$ bevat.\n",
    "\n",
    "De TF-IDF krijgen we dan door:\n",
    "\n",
    "$ \\text{tf-idf}(t,d) = \\text{tf}(t,d) \\times \\text{idf}(t)$\n",
    "\n",
    "De TF-IDF wordt berekend voor alle unieke termen $t$ in alle unieke documenten $d$. De resulterende matrix heeft dus de dimensie $\\left( t,d \\right)^{\\intercal}$ waarin element ($t_i,d_j)^{\\intercal}$ de numerieke relevantie geeft van term $i$ in document $j$.\n",
    "\n",
    "![Image of Yaktocat](http://www.jiem.org/index.php/jiem/article/viewFile/293/252/2402)\n",
    "\n",
    "#### Limitatie bag of words\n",
    "Omdat een simpel bag of words model geen rekening houdt met verkeerd gespelde woorden/typo's kunnen deze woorden als verschillend worden gezien door het algoritme. Om dit te voorkomen kunnen we gebruik maken van een collectie bigrams (n=2) of ngrams (n>2). Met een analyzer (in dit geval `char_wb`) en een gedefinieerde `ngram_range` kunnen we woorden dan herkennen aan $n$ features. Zo kunnen we voorkomen dat simpele fouten invloed hebben op het model.\n",
    "\n",
    "**Let op**: hoe meer ngrams features, hoe meer geheugen nodig is!\n",
    "\n",
    "#### Tokenizer definitie<a name=\"limitatie\"></a>\n",
    "Omdat `Tfidfvectorizer` alleen de volledige tekst als string accepteert moeten we eerst de tokens in `textdict` manipuleren -> we voegen de tokens samen tot een string en splitten die dan weer op de spatie tussen woorden. Dit is een workaround en genereert een kleine overhead.\n",
    "\n",
    "#### Overige parameters\n",
    "De standaardimplementatie van de vectorizer in de `sklearn` toolkit heeft een aantal parameters die aangepast kunnen worden, waaronder:\n",
    "1. __max_df__: dit is de maximale frequentie die een feature in documenten kan hebben om opgenomen te worden in de TF-IDF matrix. Features (woorden) met een hogere frequentie worden genegeerd. Als een term bijvoorbeeld in meer dan 80% van de documenten voorkomt heeft het waarschijnlijk weinig betekenis en/of is het lastig om aan de hand van deze feature documenten te onderscheiden.\n",
    "2. __min_idf__: minimale woordfrequentie die aangeeft in hoeveel documenten ten opzichte van de totale collectie een bepaald woord voorkomt, features met een lagere frequentie dan min_idf worden genegeerd.\n",
    "3. __ngram_range__: zie [limitatie bag of words en ngrams](#limitatie), de ngram_range (a,b) geeft aan dat alle mogelijke combinaties van woorden tussen lengte a en b gebruikt kunnen worden als feature als deze binnen de gestelde min_idf en max_idf vallen. Dit voorkomt dat verkeerde gespelde woorden, vervoegingen, etc. invloed uitoefenen op de resultaten.\n",
    "4. __use_idf__: gebruik inverse-document-frequentie\n",
    "5. __smooth_idf__: smooth idf door 1 bij de document frequentie op te tellen, dit voorkomt delen door nul en Inf errors.\n",
    "6. __sublinear_tf__: gebruik sublineaire schaling, vervang $tf$ door $1+\\log(tf)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# manipulate variable textdict to allow feeding into TfidfVectorizer function\n",
    "alltext = [' '.join(text) for text in textdict.values()]\n",
    "\n",
    "# define tokenizer\n",
    "# dummy function: we split concatenated tokens at space\n",
    "tfidf_tokenizer = lambda doc: [word for word in doc.split(' ')]\n",
    "\n",
    "# define range of ngrams\n",
    "ngrams = (3,15)\n",
    "\n",
    "# define vectorizer -- using ngrams\n",
    "sklearn_tfidf = TfidfVectorizer(analyzer='char_wb',ngram_range=ngrams,norm='l2',min_df=1,max_df=0.8,\n",
    "                                use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "# define vectorizer -- unigram\n",
    "#sklearn_tfidf = TfidfVectorizer(norm='l2',min_df=0,max_df=0.8,use_idf=True,smooth_idf=False, \n",
    "#                                sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "# calculate TFIDF matrix\n",
    "tfidf_matrix = sklearn_tfidf.fit_transform(alltext)\n",
    "\n",
    "# calculate distance\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "\n",
    "#print tfidf_matrix.toarray()[0].tolist()\n",
    "terms = sklearn_tfidf.get_feature_names()\n",
    "\n",
    "print(np.shape(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export tfidf matrix to csv\n",
    "#import pandas as pd\n",
    "\n",
    "#index = textdict.keys()\n",
    "#cols = sklearn_tfidf.get_feature_names()\n",
    "\n",
    "#df = pd.DataFrame(tfidf_matrix.todense())\n",
    "\n",
    "#df.to_csv('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print de lijst met termen. Let op, bij grote ngram_range is dit een zeer lange lijst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.shape(tfidf_matrix)[1] < 50000:\n",
    "    print terms\n",
    "else:\n",
    "    print 'List too long to print'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering en dimensiereductie\n",
    "\n",
    "Aan de hand van de TF-IDF matrix kunnen we de cosine similarity uitrekenen en de onderlinge (numerieke) afstand tussen verschillende text bestanden. Op basis van deze measure kan er dan met het KMeans algoritme een onderscheid gemaakt worden tussen verschillende clusters.\n",
    "\n",
    "Het aantal clusters moeten we zelf definieren. Dit is onderhevig aan trial en error: het zoeken van het optimale aantal clusters is geen triviaal proces en wordt vaak aangepakt met een brute force aanpak. Dit komt dus neer op $m$ keer het algoritme zijn clusters laten bepalen voor $n$ verschillende clustergrootten en achteraf bepalen met welke clustergrootte het beste resultaat wordt bereikt.\n",
    "\n",
    "KMeans clusters door documenten zo te groeperen waardoor de variantie (statistische spreiding) van de groep minimaal is. Dit kan op twee manieren benaderd worden:\n",
    "\n",
    "1. KMeans direct toepassen op een $n$-dimensionale dataset. Dit verdeelt de set met documenten over clusters maar visualisatie is niet direct mogelijk.\n",
    "2. De dataset eerst reduceren naar het tweedimensionale vlak met PCA (mathematisch) en vervolgens KMeans toepassen. Dit staat wel toe om de clusters grafisch te representeren.\n",
    "\n",
    "Naast de bovengenoemde benaderingen passen we ook nog een probabilistische dimensiereductiemethode toe:\n",
    "3. De dataset eerst reduceren naar tweedimensionaal vlak met t-SNE (probabilistisch) en vervolgens KMeans toepassen.\n",
    "\n",
    "Voor alle clusteringmethodes gebruiken we de volgende settings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 11 # number of clusters\n",
    "max_iter = 10000 # maximum number of iterations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KMeans op $n$-dimensionale dataset\n",
    "\n",
    "We passen eerst KMeans direct toe op de $n$-dimensionale dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "name = \"kmeans\"\n",
    "run_settings[name] = True\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=10000)\n",
    "km.fit(tfidf_matrix)\n",
    "\n",
    "# print clusters\n",
    "for i in np.arange(0,num_clusters):\n",
    "    print \"Cluster\", i, \": \\n\"\n",
    "    print np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "clusters = km.labels_.tolist()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"         K-MEANS CLUSTERING         \\n\")\n",
    "    text_file.write(\"====================================\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        text_file.write(\"Cluster {}\\n\".format(i))\n",
    "        text_file.write(\"{}\".format(np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]))\n",
    "        text_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eerst dimensiereductie PCA dan KMeans\n",
    "\n",
    "Alvorens te clusteren met het KMeans algoritme reduceren we het aantal dimensies eerst naar $n=2$ met behulp van PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# set name\n",
    "name = 'kmeans_pca'\n",
    "run_settings[name] = True\n",
    "\n",
    "# first reduce dimensions to two\n",
    "pca = PCA(n_components=2).fit(tfidf_matrix.todense())\n",
    "X = pca.transform(tfidf_matrix.todense()) # transformed data\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=max_iter)\n",
    "km.fit(X)\n",
    "\n",
    "# print clusters\n",
    "for i in np.arange(0,num_clusters):\n",
    "    print \"Cluster\", i, \": \\n\"\n",
    "    print np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"       K-MEANS PCA CLUSTERING       \\n\")\n",
    "    text_file.write(\"====================================\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        text_file.write(\"Cluster {}\\n\".format(i))\n",
    "        text_file.write(\"{}\".format(np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]))\n",
    "        text_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eerst dimensiereductie t-SNE daarna KMeans\n",
    "\n",
    "Alvorens te clusteren met het KMeans algoritme reduceren we het aantal dimensies eerst naar $n=2$ met behulp van t-SNE. t-SNE is, in tegenstelling tot PCA (mathematisch), een probabilistische methode om de dimensies te reduceren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# set name\n",
    "name = 'kmeans_tsne'\n",
    "run_settings[name] = True\n",
    "\n",
    "# first reduce dimensions to two\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=20, n_iter=10000)\n",
    "X = tsne.fit_transform(tfidf_matrix.todense()) # transformed data\n",
    "\n",
    "km = KMeans(n_clusters=num_clusters, max_iter=max_iter)\n",
    "km.fit(X)\n",
    "\n",
    "# print clusters\n",
    "for i in np.arange(0,num_clusters):\n",
    "    print \"Cluster\", i, \": \\n\"\n",
    "    print np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]\n",
    "    print \"\\n\\n\"\n",
    "    \n",
    "clusters = km.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"      K-MEANS t-SNE CLUSTERING      \\n\")\n",
    "    text_file.write(\"====================================\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        text_file.write(\"Cluster {}\\n\".format(i))\n",
    "        text_file.write(\"{}\".format(np.array(textdict.keys())[np.array(km.labels_.tolist()) == i]))\n",
    "        text_file.write(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster visualisatie\n",
    "\n",
    "Gebruik de onderstaande scripts om per clustermethode (na dimensiereductie) de clusters te visualiseren. `X` is de set getransformeerde data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize cluster boundaries\n",
    "\n",
    "# Step size of the mesh. Decrease to increase the quality of the VQ.\n",
    "h = np.max([np.abs(X[:,0].min()), np.abs(X[:,0].max())])/200   \n",
    "\n",
    "# Plot the decision boundary. For that, we will assign a color to each\n",
    "x_min, x_max = X[:, 0].min(), X[:, 0].max()\n",
    "y_min, y_max = X[:, 1].min(), X[:, 1].max()\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "\n",
    "# Obtain labels for each point in mesh. Use last trained model.\n",
    "Z = km.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(figsize=(16,12))\n",
    "plt.clf()\n",
    "plt.imshow(Z, interpolation='nearest',\n",
    "           extent=(xx.min(), xx.max(), yy.min(), yy.max()),\n",
    "           cmap=plt.cm.Paired,\n",
    "           aspect='auto', origin='lower')\n",
    "\n",
    "plt.plot(X[:, 0], X[:, 1], 'k.', markersize=20)\n",
    "\n",
    "# Plot the centroids as a white X\n",
    "centroids = km.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1],\n",
    "            marker='x', s=100, linewidths=3,\n",
    "            color='w', zorder=10)\n",
    "plt.title('K-means clustering on the dataset (' + name + ')' + '\\n'\n",
    "          'Centroids are marked with white cross', fontsize=16)\n",
    "plt.xlim([np.min(X[:, 0]),np.max(X[:, 0])])\n",
    "plt.ylim([np.min(X[:, 1]),np.max(X[:, 1])])\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "# save figure\n",
    "plt.savefig('./Output/Rapporten-boundaries-' + name + ' ' + run_settings['datetime'] + '.pdf')\n",
    "run_settings[name] = './Output/Rapporten-boundaries-' + name + ' ' + run_settings['datetime'] + '.pdf'\n",
    "\n",
    "# plot figure\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize clusters\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.offline as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "# get 2-dimensional data\n",
    "xs, ys = X[:, 0], X[:, 1]\n",
    "\n",
    "# visualize clusters\n",
    "df = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=textdict.keys())) \n",
    "groups = df.groupby('label')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8)) # set size\n",
    "ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n",
    "\n",
    "cluster_names = ['Cluster ' + str(w) for w in np.arange(np.min(clusters)+1,np.max(clusters)+2,1)]\n",
    "np.random.RandomState = 0\n",
    "cluster_colors = plt.cm.jet(np.linspace(0,0.9,num_clusters))\n",
    "\n",
    "\n",
    "for cname, group in groups:\n",
    "    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n",
    "            label=cluster_names[cname], color=cluster_colors[cname], \n",
    "            mec='none')\n",
    "    ax.set_aspect('auto')\n",
    "    \n",
    "# plot centroid numbers\n",
    "centroids = km.cluster_centers_\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], marker='o',\n",
    "                c=\"white\", alpha=0.2, s=400, edgecolor='k')\n",
    "for i in np.arange(0,num_clusters):\n",
    "    plt.text(centroids[i, 0]-0.006, centroids[i, 1]-0.015, str(i+1))\n",
    "    \n",
    "plt.title('K-means clustering on the dataset (' + name + ')' + '\\n',\n",
    "          fontsize=16)\n",
    "\n",
    "# convert plot object to plotly object\n",
    "fig = tools.mpl_to_plotly(fig)\n",
    "\n",
    "options = {\"displayModeBar\": True,\"modeBarButtonsToRemove\": ['sendDataToCloud'],\"showLink\": False,\n",
    "           \"displaylogo\": False, \"setBackground\": \"transparent\"} \n",
    "\n",
    "# set legend and layout options\n",
    "fig['layout']['showlegend'] = True\n",
    "fig['layout']['xaxis1']['ticks'] = ''\n",
    "fig['layout']['xaxis1']['showticklabels'] = False\n",
    "fig['layout']['yaxis1']['ticks'] = ''\n",
    "fig['layout']['yaxis1']['showticklabels'] = False\n",
    "fig['data'][-1]['name'] = 'Centroids'\n",
    "fig['data'][-1]['text'] = 'Cluster center'\n",
    "\n",
    "# set plot hover text\n",
    "for i in np.arange(0,num_clusters):\n",
    "    fig['data'][i]['text'] = np.array(textdict.keys())[np.array(km.labels_.tolist()) == i].tolist()\n",
    "\n",
    "# plot and embed in ipython notebook\n",
    "py.iplot(fig, config=options)\n",
    "\n",
    "# save to file, uncomment to save\n",
    "run_settings[name] = './Output/Rapporten-clusters-' + name + ' ' + run_settings['datetime'] + '.html'\n",
    "py.plot(fig, config=options, filename='./Output/Rapporten-clusters-' + name + ' ' + run_settings['datetime'] + '.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clusteroptimalisatie\n",
    "\n",
    "Bij de bovenstaande clustermethodes is gebruik gemaakt van een vooraf bepaald aantal clusters. Clusteroptimalisatie, het bepalen van het optimale aantal clusters, is nog een van de openstaande vraagstukken in de statistiek. Er zijn verschillende methodes om het optimale aantal clusters te bepalen. Wij maken hier gebruik van silhouetteanalyse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]\n",
    "\n",
    "# Create a subplot with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(np.shape(range_n_clusters)[0], 2)\n",
    "fig.set_size_inches(22, 15*np.shape(range_n_clusters)[0])\n",
    "\n",
    "for n_clusters in range_n_clusters:\n",
    "    \n",
    "    ind = sorted(range_n_clusters).index(n_clusters)\n",
    "\n",
    "    # The 1st subplot is the silhouette plot\n",
    "    # The silhouette coefficient can range from -1, 1 but in this example all\n",
    "    # lie within [-0.1, 1]\n",
    "    axs[ind][0].set_xlim([-0.1, 1])\n",
    "    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n",
    "    # plots of individual clusters, to demarcate them clearly.\n",
    "    axs[ind][0].set_ylim([0, len(X) + (n_clusters + 1) * 10])\n",
    "\n",
    "    # Initialize the clusterer with n_clusters value and a random generator\n",
    "    # seed of 10 for reproducibility.\n",
    "    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "    cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    # The silhouette_score gives the average value for all the samples.\n",
    "    # This gives a perspective into the density and separation of the formed\n",
    "    # clusters\n",
    "    silhouette_avg = silhouette_score(X, cluster_labels)\n",
    "    #print(\"For n_clusters =\", n_clusters,\n",
    "    #      \"The average silhouette_score is :\", silhouette_avg)\n",
    "\n",
    "    # Compute the silhouette scores for each sample\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = \\\n",
    "            sample_silhouette_values[cluster_labels == i]\n",
    "\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.spectral(float(i) / n_clusters)\n",
    "        axs[ind][0].fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        axs[ind][0].text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    axs[ind][0].set_title(\"Silhouette analysis for KMeans ({}) clustering on sample data\"\n",
    "                  \"with n_clusters = {}\".format(name,n_clusters))\n",
    "    axs[ind][0].set_xlabel(\"The silhouette coefficient values\")\n",
    "    axs[ind][0].set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    axs[ind][0].axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    axs[ind][0].set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    axs[ind][0].set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    axs[ind][1].scatter(X[:, 0], X[:, 1], marker='.', s=50, lw=0, alpha=0.7,\n",
    "                c=colors, edgecolor='k')\n",
    "    \n",
    "    #add label in x,y position with the label as the film title\n",
    "    for j in range(len(df)):\n",
    "        axs[ind][1].text(X[j, 0]*1.05, X[j, 1], df.iloc[j]['title'], size=6)  \n",
    "\n",
    "    # Labeling the clusters\n",
    "    centers = clusterer.cluster_centers_\n",
    "    # Draw white circles at cluster centers\n",
    "    axs[ind][1].scatter(centers[:, 0], centers[:, 1], marker='o',\n",
    "                c=\"white\", alpha=1, s=200, edgecolor='k')\n",
    "\n",
    "    for i, c in enumerate(centers):\n",
    "        axs[ind][1].scatter(c[0], c[1], marker='$%d$' % i, alpha=1,\n",
    "                    s=50, edgecolor='k')\n",
    "\n",
    "    axs[ind][1].set_title(\"The visualization of the clustered data.\")\n",
    "    axs[ind][1].set_xlabel(\"Feature space for the 1st feature\")\n",
    "    axs[ind][1].set_ylabel(\"Feature space for the 2nd feature\")\n",
    "\n",
    "    #plt.suptitle((\"Silhouette analysis for KMeans clustering on sample data \"\n",
    "    #              \"with n_clusters = %d\" % n_clusters),\n",
    "    #             fontsize=14, fontweight='bold')\n",
    "    \n",
    "fig.savefig('./Output/Rapport-clusteranalysis' + name + '.pdf')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical document clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import ward, dendrogram, linkage\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "dist = 1 - cosine_similarity(tfidf_matrix)\n",
    "linkage_matrix = linkage(dist, 'ward') # ward/single/average/complete\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 15./60*len(textdict.keys())))\n",
    "\n",
    "ax = dendrogram(linkage_matrix, orientation=\"right\", labels=textdict.keys());\n",
    "\n",
    "plt.tick_params(\\\n",
    "    axis= 'x',          # changes apply to the x-axis\n",
    "    which='both',      # both major and minor ticks are affected\n",
    "    bottom='off',      # ticks along the bottom edge are off\n",
    "    top='off',         # ticks along the top edge are off\n",
    "    labelbottom='off')\n",
    "\n",
    "plt.tight_layout() #show plot with tight layout\n",
    "plt.savefig('./Output/Rapport-dendogram_plot ' + run_settings['datetime'] + '.pdf')\n",
    "run_settings['dendogram'] = './Output/Rapport-dendogram_plot ' + run_settings['datetime'] + '.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation - Topic Modeling\n",
    "\n",
    "Om meer van de verborgen structuur in de tekstdocumenten te vinden kunnen we gebruik maken van _Latend Dirichlet Allocation_. LDA is een propabilistisch topic model die aanneemt dat documenten een mix zijn van verschillende onderwerpen (topics) en dat elk woord in het document bijdraagt aan dit onderwerp (en zo komt het onderwerp tot stand).\n",
    "\n",
    "![Image](http://mengjunxie.github.io/ae-lda/img/IntroToLDA.png)\n",
    "\n",
    "Om LDA te gebruiken moeten we eerst een model trainen met de data (teksten) die we hebben.\n",
    "\n",
    "#### LDA model parameters\n",
    "Voor het trainen van een LDA model gebruiken we de standaard implementatie van `gensim`. In dit model zijn een aantal aan te passen parameters:\n",
    "\n",
    "1. __no_below__: houd tokens die minstens in dit aantal documenten aanwezig zijn.\n",
    "2. __no_above__: verwijder tokens die in dit aantal (fractie van totaal) documenten aanwezig zijn.\n",
    "3. __num_topics__: aantal te identificeren topic clusters. Dit is een van de belangrijkste parameters.\n",
    "4. __update_every__: aantal chunks die verwerkt worden voordat de stap wordt gemaakt van E naar M (EM: Expectation Maximization).\n",
    "5. __chunk_size__: aantal documenten die tegelijkertijd in geheugen geladen wordt.\n",
    "\n",
    "Voor meer info: https://miningthedetails.com/blog/python/lda/GensimLDA/\n",
    "\n",
    "Eerst importeren we de benodigde toolkits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maken nu een dictionary van alle tokens in onze `textdict` variabele."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Gensim dictionary from the texts\n",
    "dictionary = corpora.Dictionary(textdict.values())\n",
    "\n",
    "# remove extremes\n",
    "dictionary.filter_extremes(no_below=2, no_above=0.8)\n",
    "\n",
    "# convert dictionary to a bag of words corpus for reference\n",
    "corpus = [dictionary.doc2bow(text) for text in textdict.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_topics = 11\n",
    "\n",
    "# train the LDA model - TAKES A LONG TIME!\n",
    "%time lda = models.LdaModel(corpus, num_topics=n_topics,id2word = dictionary,update_every=1,chunksize=100000,passes=50)\n",
    "\n",
    "# save lda model\n",
    "lda.save('./Output/lda '+ run_settings['datetime'] +'.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laat de eerste 30 woorden zien van elk topic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_matrix = lda.show_topics(formatted=False, num_words=30)\n",
    "\n",
    "# show topics\n",
    "topics_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"  LATENT DIRICHLET ALLOCATION model \\n\")\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"Training set feature scores\\n\\n\")\n",
    "    for i in np.arange(0,len(topics_matrix)):\n",
    "        text_file.write(\"Cluster {}\\n\".format(topics_matrix[i][0]))\n",
    "        text_file.write(str(topics_matrix[i][1]))\n",
    "        text_file.write(\"\\n\\n\")\n",
    "    text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document classificeren met LDA\n",
    "\n",
    "Om nu nieuwe documenten, die niet gebruikt zijn bij het trainen van bovenstaand LDA model (en dus niet deel uitmaken van de standaard corpus), te classificeren met LDA, laden we eerst de desbetreffende documenten en tokenizen deze met de eerder gedefinieerde `processandtokenize` functie. Indien deze functie nog niet geladen is, run [deze](#processandtokenize) cell.\n",
    "\n",
    "Daarna kunnen we de documenten invoeren in het LDA model. Hier komt vervolgens een score uit. In het geval een document scores behaalt binnen meerdere LDA clusters dan worden al deze scores gereturned. Als stopwords zijn verwijderd en de text 'gestemd' is voor de trainingsset doen we dat ook voor de validatieset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilePath_val = './Rapporten selectie/validatie/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames_val = os.listdir(FilePath_val)\n",
    "\n",
    "# Preallocate variables\n",
    "textdict_val = {}\n",
    "metadict_val = {}\n",
    "\n",
    "for i in np.arange(np.shape(FileNames_val)[0]): # use this line for all files\n",
    "    print \"Processing \",i+1,'/',np.shape(FileNames_val)[0],': ', FileNames_val[i],'\\n'\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath_val,FileNames_val[i])\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath_val,FileNames_val[i]]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict_val[FileNames_val[i]] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        # filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        # filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "            # add data to dict\n",
    "            textdict_val[FileNames_val[i]] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileNames_val[i]\n",
    "        \n",
    "# remove stopwords and stem text\n",
    "if run_settings['stopwords_removed'] == True:\n",
    "    removestopwords(textdict_val)\n",
    "if run_settings['text_stemmed'] == True:\n",
    "    stemtext(textdict_val)\n",
    "        \n",
    "# Create validation dictionary and corpus\n",
    "dictionary_val = corpora.Dictionary(textdict_val.values())\n",
    "corpus_val = [dictionary_val.doc2bow(text) for text in textdict_val.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classify texts and print results\n",
    "for i in np.arange(0,np.shape(FileNames_val)[0]):\n",
    "    topic_ids = sorted(lda[corpus_val[i]], key=lambda (index, score): -score)\n",
    "    print 'LDA cluster      score'\n",
    "    for j in topic_ids:\n",
    "        print int(j[0]), '              ', j[1]\n",
    "    print '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output clusters to text file\n",
    "with open(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", \"a\") as text_file:\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"     LATENT DIRICHLET ALLOCATION    \\n\")\n",
    "    text_file.write(\"====================================\\n\")\n",
    "    text_file.write(\"Validation set classification cluster scores\\n\\n\")\n",
    "    for i in np.arange(0,num_clusters):\n",
    "        topic_ids = sorted(lda[corpus_val[i]], key=lambda (index, score): -score)\n",
    "        text_file.write(FileNames_val[i]+'\\n')\n",
    "        text_file.write(\"LDA cluster      score\\n\")\n",
    "        for j in topic_ids:\n",
    "            text_file.write(str(int(j[0])) + '                ' + str(j[1]) + \"\\n\")\n",
    "        text_file.write(\"\\n\")\n",
    "    text_file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finaliseer logbestand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to add text to beginning of file\n",
    "def line_prepender(filename, line):\n",
    "    with open(filename, 'r+') as f:\n",
    "        content = f.read()\n",
    "        f.seek(0, 0)\n",
    "        f.write(line.rstrip('\\r\\n') + '\\n' + content)\n",
    "        \n",
    "# add title and settings info\n",
    "head_settings = \"\"\n",
    "for k,v in run_settings.items():\n",
    "    head_settings = head_settings + str(k) + ': ' + str(v) + \"\\n\"\n",
    "header = run_settings['title'] + \"\\n\\n\" + \"====================================\\n\" + \\\n",
    "         \"             RUN SETTINGS             \\n\" + \"====================================\\n\\n\" + \\\n",
    "         head_settings + \"\\n\\n\"\n",
    "\n",
    "line_prepender(\"./Output/Run logfile \"+run_settings[\"datetime\"]+\".txt\", header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workspace variabelen opslaan om later te gebruiken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./Vars/lang.npy', lang)\n",
    "np.save('./Vars/filepath.npy', FilePath)\n",
    "np.save('./Vars/filenames.npy', FileNames)\n",
    "np.save('./Vars/run_settings.npy', run_settings)\n",
    "np.save('./Vars/textdict.npy', textdict)\n",
    "np.save('./Vars/metadict.npy', metadict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot tijdlijn documenten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "date = np.array([])\n",
    "\n",
    "# convert metadict date field to date array\n",
    "for i in np.arange(0,len(metadict)):\n",
    "    year = metadict[metadict.keys()[i]]['/CreationDate'][2:6]\n",
    "    month = metadict[metadict.keys()[i]]['/CreationDate'][6:8]\n",
    "    day = metadict[metadict.keys()[i]]['/CreationDate'][8:10]\n",
    "\n",
    "    # convert to python datetime object\n",
    "    date = np.append(date,datetime.date(int(year), int(month), int(day)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot dates\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import dates as pltdates\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "# set plot properties\n",
    "plt.title(\"Tijdlijn rapporten\", fontsize=22)\n",
    "fig.set_size_inches(20,20./60*len(textdict.keys()))\n",
    "fig.gca().spines['left'].set_visible(False)\n",
    "fig.gca().spines['right'].set_visible(False)\n",
    "fig.gca().spines['top'].set_visible(False)\n",
    "fig.gca().get_yaxis().set_ticks([]) # remove y ticks\n",
    "fig.gca().axes.yaxis.set_ticklabels([]) # remove y labels\n",
    "plt.grid(True,which='major',axis='x')\n",
    "\n",
    "# plot data\n",
    "xplot = np.arange(0,np.shape(textdict.keys())[0])\n",
    "yind = np.argsort(date) # sort date for plotting\n",
    "plt.scatter(date[yind], xplot, c=(75./256,151./256,182./256),s=100) \n",
    "for i in np.arange(0,np.shape(textdict.keys())[0]):\n",
    "    plt.text(date[yind[i]]+datetime.timedelta(60), i, metadict.keys()[yind[i]], fontsize=8)\n",
    "    \n",
    "plt.xticks(np.arange(datetime.date(min(date).year,1,1),datetime.date(max(date).year+1,1,1),365),fontsize=16,rotation=70)\n",
    "    \n",
    "# make sure text fits in axis\n",
    "plt.xlim(fig.gca().get_xlim()[0],fig.gca().get_xlim()[1]*1.003)\n",
    "\n",
    "# show figure and save figure to working directory\n",
    "plt.savefig('./Output/Rapport-datum_plot.pdf', dpi=80, orientation='landscape')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IDEE: toekomstige implementaties\n",
    "\n",
    "Tekst met keywords in pdf markeren, zie voorbeelden om locatie van het woord te extracten en de tekst in het PDF bestand te highlighten:\n",
    "\n",
    "- https://stackoverflow.com/questions/7605577/read-highlight-save-pdf-programatically\n",
    "- https://stackoverflow.com/questions/22898145/how-to-extract-text-and-text-coordinates-from-a-pdf-file/\n",
    "\n",
    "\n",
    "Gebruik PCA of t-SNE bij clusteren\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA van sklearn\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# manipulate variable textdict to allow feeding into TfidfVectorizer function\n",
    "alltext = [' '.join(text) for text in textdict.values()]\n",
    "\n",
    "# define tokenizer\n",
    "# dummy function: we split concatenated tokens at space\n",
    "tf_tokenizer = lambda doc: [word for word in doc.split(' ')]\n",
    "\n",
    "# define range of ngrams\n",
    "ngrams = (3,15)\n",
    "\n",
    "# define vectorizer -- using ngrams\n",
    "#sklearn_tfidf = TfidfVectorizer(analyzer='char_wb',ngram_range=ngrams,norm='l2',min_df=1,max_df=0.8,\n",
    "#                                use_idf=True,smooth_idf=False, sublinear_tf=True, tokenizer=tfidf_tokenizer)\n",
    "\n",
    "sklearn_tf = CountVectorizer(max_df=0.95, min_df=2, tokenizer=tf_tokenizer, analyzer='char_wb',\n",
    "                             ngram_range=ngrams)\n",
    "tf_matrix = sklearn_tf.fit_transform(alltext)\n",
    "\n",
    "#print tfidf_matrix.toarray()[0].tolist()\n",
    "tf_feature_names = sklearn_tf.get_feature_names()\n",
    "\n",
    "#print(np.shape(tfidf_matrix))\n",
    "np.shape(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "\n",
    "n_topics = 10\n",
    "\n",
    "nmf = NMF(n_components=n_topics, random_state=1, alpha=.1, l1_ratio=.5,\n",
    "          init='nndsvd').fit(tfidf_matrix)\n",
    "\n",
    "lda = LatentDirichletAllocation(n_components=n_topics, max_iter=5, learning_method='online',\n",
    "                                learning_offset=50., random_state=0).fit(tf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print \"Topic %d:\" % (topic_idx)\n",
    "        print \" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]])\n",
    "\n",
    "no_top_words = 20\n",
    "display_topics(nfm, terms, no_top_words)\n",
    "display_topics(lda, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FilePath_val = './Rapporten selectie/validatie/'\n",
    "\n",
    "# Get files in folder\n",
    "FileNames_val = os.listdir(FilePath_val)\n",
    "\n",
    "# Preallocate variables\n",
    "textdict_val = {}\n",
    "metadict_val = {}\n",
    "\n",
    "for i in np.arange(np.shape(FileNames_val)[0]): # use this line for all files\n",
    "    print \"Processing \",i+1,'/',np.shape(FileNames_val)[0],': ', FileNames_val[i],'\\n'\n",
    "    \n",
    "    try:\n",
    "        data = pdfparser(FilePath_val,FileNames_val[i])\n",
    "        \n",
    "        # get PDF metadata\n",
    "        pdfi = PdfFileReader(open(\"\".join([FilePath_val,FileNames_val[i]]), \"rb\"))\n",
    "        pdfi = pdfi.getDocumentInfo()\n",
    "        metadict_val[FileNames_val[i]] = pdfi\n",
    "\n",
    "        # split text\n",
    "        data = np.array(re.split('\\W+',data),dtype=str) # tokenize text\n",
    "        \n",
    "        # convert all data to lower case\n",
    "        data = np.array([w.lower() for w in data])\n",
    "\n",
    "        # filter out empty cells\n",
    "        ind = data != ''\n",
    "        data = data[ind]\n",
    "\n",
    "        # filter out non-alpha strings\n",
    "        ind = np.char.isalpha(data)\n",
    "        data = data[ind]\n",
    "\n",
    "        if data.size is not 0:\n",
    "            # filter out strings with length = 1\n",
    "            strlen = np.vectorize(len)\n",
    "            ind = strlen(data) != 1\n",
    "            data = data[ind]\n",
    "        \n",
    "            # add data to dict\n",
    "            textdict_val[FileNames_val[i]] = list(data)\n",
    "\n",
    "    except ValueError:\n",
    "        print \"Skipping \", FileNames_val[i]\n",
    "        \n",
    "# remove stopwords and stem text\n",
    "if run_settings['stopwords_removed'] == True:\n",
    "    removestopwords(textdict_val)\n",
    "if run_settings['text_stemmed'] == True:\n",
    "    stemtext(textdict_val)\n",
    "        \n",
    "# Create validation dictionary and corpus\n",
    "dictionary_val = corpora.Dictionary(textdict_val.values())\n",
    "corpus_val = [dictionary_val.doc2bow(text) for text in textdict_val.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# classify unseen text\n",
    "alltext_val = [' '.join(text) for text in textdict_val.values()]\n",
    "tf_matrix_val = sklearn_tf.transform(alltext_val)\n",
    "val_topics = lda.transform(tf_matrix_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
